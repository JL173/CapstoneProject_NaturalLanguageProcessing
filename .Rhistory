}
index =  index + 1
if ((index %% 50) == 0){
percent_complete <- index / length(test_strings)
print(index / length(test_strings))
total_time <- (Sys.time() - start_time) / percent_complete
time_remaining <- total_time * (1 - percent_complete)
print(time_remaining)
}
result <- Predict(string,
model = blogs_train_model,
ngram = 2,
gamma = 0.3) %>%
head(1)
result_words <- c(result_words, result[1, "word"])
result_P <- c(result_P, result[1, "P"])
}
test_blogs_trigrams$predictions <- result_words
test_blogs_trigrams$probability <- result_P
n <- Sys.time()
t <- n - t
print(t)
save(test_blogs_trigrams, file = "output/test_blogs_trigrams_5.RDa")
t <- Sys.time()
test_copy <- test_blogs_quagrams
test_strings <- test_copy[["string"]]
result_words <- c()
result_P <- c()
index = 0
for (string in test_strings){
if (index == 0){
start_time <- Sys.time()
}
index =  index + 1
if ((index %% 50) == 0){
percent_complete <- index / length(test_strings)
print(index / length(test_strings))
total_time <- (Sys.time() - start_time) / percent_complete
time_remaining <- total_time * (1 - percent_complete)
print(time_remaining)
}
result <- Predict(string,
model = blogs_train_model,
ngram = 3,
gamma = 0.3) %>%
head(1)
result_words <- c(result_words, result[1, "word"])
result_P <- c(result_P, result[1, "P"])
}
test_blogs_quagrams$predictions <- result_words
test_blogs_quagrams$probability <- result_P
n <- Sys.time()
t <- n - t
print(t)
save(test_blogs_quagrams, file = "output/test_blogs_quagrams_5.RDa")
base <- LoadSingleFile(
"data/IMDB/neg_merged.txt",
SOS = TRUE, EOS = TRUE)
imdb_neg_unigrams <- CleanTokens(base, n = 1) %>%
WordFreq() %>%
filter(freq > 2)
imdb_neg_bigrams <- CleanTokens(base, n = 2) %>%
WordFreq() %>%
filter(freq > 1)
imdb_neg_trigrams <- CleanTokens(base, n = 3) %>%
WordFreq() %>%
filter(freq > 1)
imdb_neg_quagrams <- CleanTokens(base, n = 4) %>%
WordFreq() %>%
filter(freq > 1)
TrainTestSplit <- function(df, percent = 0.95, seed = 17373){
smp_size <- floor(percent * nrow(df))
set.seed(seed)
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
train <- df[train_ind, ]
test <- df[-train_ind, ]
results <- list(train = train, test = test)
}
trigrams_ <-TrainTestSplit(imdb_neg_trigrams, 0.95)
test_imdb_neg_trigrams <- trigrams_$test
train_imdb_neg_trigrams <- trigrams_$train
rm(trigrams_)
quagrams_ <-TrainTestSplit(imdb_neg_quagrams, 0.95)
test_imdb_neg_quagrams <- quagrams_$test
train_imdb_neg_quagrams <- quagrams_$train
rm(quagrams_)
imdb_neg_train_model <- CreateFreqTable(imdb_neg_unigrams,
imdb_neg_bigrams,
train_imdb_neg_trigrams,
train_imdb_neg_quagrams)
#save(imdb_neg_train_model, file = "models/imdb_neg_train_model.RDa")
RemoveTestDuplicates <- function(df, ngram){
if (ngram == 3){
df <- df %>%
separate(word, c("word1", "word2", "word3"), sep = " ") %>%
select(c("word1", "word2", "word3", "freq"))
df[, "word4"] <- NA
df <- distinct(df, word1, word2, .keep_all = TRUE)
} else if (ngram == 4){
df <- df %>%
separate(word, c("word1", "word2", "word3", "word4"), sep = " ") %>%
select(c("word1", "word2", "word3", "word4", "freq"))
df <- distinct(df, word1, word2, word3, .keep_all = TRUE)
}
df
}
test_imdb_neg_trigrams <- test_imdb_neg_trigrams %>%
RemoveTestDuplicates(ngram = 3) %>%
filter(!is.na(word1))
test_imdb_neg_quagrams <- test_imdb_neg_quagrams %>%
RemoveTestDuplicates(ngram = 4) %>%
filter(!is.na(word1))
CreatePredictionString <- function(df, ngram){
if (ngram == 3){
df <- df %>%
unite(col = string, word1, word2, sep = " ",
na.rm = TRUE, remove = FALSE)
} else if (ngram == 4){
df <- df %>%
unite(col = string, word1, word2, word3, sep = " ",
na.rm = TRUE, remove = FALSE)
}
}
test_imdb_neg_trigrams <- test_imdb_neg_trigrams %>%
CreatePredictionString(ngram = 3)
test_imdb_neg_quagrams <- test_imdb_neg_quagrams %>%
CreatePredictionString(ngram = 4)
t <- Sys.time()
test_copy <- test_imdb_neg_trigrams
test_strings <- test_copy[["string"]]
result_words <- c()
result_P <- c()
index = 0
for (string in test_strings){
if (index == 0){
start_time <- Sys.time()
}
index =  index + 1
if ((index %% 50) == 0){
percent_complete <- index / length(test_strings)
print(index / length(test_strings))
total_time <- (Sys.time() - start_time) / percent_complete
time_remaining <- total_time * (1 - percent_complete)
print(time_remaining)
}
result <- Predict(string,
model = imdb_neg_train_model,
ngram = 2,
gamma = 0.3) %>%
head(1)
result_words <- c(result_words, result[1, "word"])
result_P <- c(result_P, result[1, "P"])
}
test_imdb_neg_trigrams$predictions <- result_words
test_imdb_neg_trigrams$probability <- result_P
n <- Sys.time()
t <- n - t
print(t)
save(test_imdb_neg_trigrams, file = "output/test_imdb_neg_trigrams_5.RDa")
t <- Sys.time()
test_copy <- test_imdb_neg_quagrams
test_strings <- test_copy[["string"]]
result_words <- c()
result_P <- c()
index = 0
for (string in test_strings){
if (index == 0){
start_time <- Sys.time()
}
index =  index + 1
if ((index %% 50) == 0){
percent_complete <- index / length(test_strings)
print(index / length(test_strings))
total_time <- (Sys.time() - start_time) / percent_complete
time_remaining <- total_time * (1 - percent_complete)
print(time_remaining)
}
result <- Predict(string,
model = imdb_neg_train_model,
ngram = 3,
gamma = 0.3) %>%
head(1)
result_words <- c(result_words, result[1, "word"])
result_P <- c(result_P, result[1, "P"])
}
test_imdb_neg_quagrams$predictions <- result_words
test_imdb_neg_quagrams$probability <- result_P
n <- Sys.time()
t <- n - t
print(t)
save(test_imdb_neg_quagrams, file = "output/test_imdb_neg_quagrams_5.RDa")
base <- LoadSingleFile(
"data/IMDB/pos_merged.txt",
SOS = TRUE, EOS = TRUE)
imdb_pos_unigrams <- CleanTokens(base, n = 1) %>%
WordFreq() %>%
filter(freq > 2)
imdb_pos_bigrams <- CleanTokens(base, n = 2) %>%
WordFreq() %>%
filter(freq > 1)
imdb_pos_trigrams <- CleanTokens(base, n = 3) %>%
WordFreq() %>%
filter(freq > 1)
imdb_pos_quagrams <- CleanTokens(base, n = 4) %>%
WordFreq() %>%
filter(freq > 1)
TrainTestSplit <- function(df, percent = 0.95, seed = 17373){
smp_size <- floor(percent * nrow(df))
set.seed(seed)
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
train <- df[train_ind, ]
test <- df[-train_ind, ]
results <- list(train = train, test = test)
}
trigrams_ <-TrainTestSplit(imdb_pos_trigrams, 0.95)
test_imdb_pos_trigrams <- trigrams_$test
train_imdb_pos_trigrams <- trigrams_$train
rm(trigrams_)
quagrams_ <-TrainTestSplit(imdb_pos_quagrams, 0.95)
test_imdb_pos_quagrams <- quagrams_$test
train_imdb_pos_quagrams <- quagrams_$train
rm(quagrams_)
imdb_pos_train_model <- CreateFreqTable(imdb_pos_unigrams,
imdb_pos_bigrams,
train_imdb_pos_trigrams,
train_imdb_pos_quagrams)
#save(imdb_pos_train_model, file = "models/imdb_pos_train_model.RDa")
RemoveTestDuplicates <- function(df, ngram){
if (ngram == 3){
df <- df %>%
separate(word, c("word1", "word2", "word3"), sep = " ") %>%
select(c("word1", "word2", "word3", "freq"))
df[, "word4"] <- NA
df <- distinct(df, word1, word2, .keep_all = TRUE)
} else if (ngram == 4){
df <- df %>%
separate(word, c("word1", "word2", "word3", "word4"), sep = " ") %>%
select(c("word1", "word2", "word3", "word4", "freq"))
df <- distinct(df, word1, word2, word3, .keep_all = TRUE)
}
df
}
test_imdb_pos_trigrams <- test_imdb_pos_trigrams %>%
RemoveTestDuplicates(ngram = 3) %>%
filter(!is.na(word1))
test_imdb_pos_quagrams <- test_imdb_pos_quagrams %>%
RemoveTestDuplicates(ngram = 4) %>%
filter(!is.na(word1))
CreatePredictionString <- function(df, ngram){
if (ngram == 3){
df <- df %>%
unite(col = string, word1, word2, sep = " ",
na.rm = TRUE, remove = FALSE)
} else if (ngram == 4){
df <- df %>%
unite(col = string, word1, word2, word3, sep = " ",
na.rm = TRUE, remove = FALSE)
}
}
test_imdb_pos_trigrams <- test_imdb_pos_trigrams %>%
CreatePredictionString(ngram = 3)
test_imdb_pos_quagrams <- test_imdb_pos_quagrams %>%
CreatePredictionString(ngram = 4)
t <- Sys.time()
test_copy <- test_imdb_pos_trigrams
test_strings <- test_copy[["string"]]
result_words <- c()
result_P <- c()
index = 0
for (string in test_strings){
if (index == 0){
start_time <- Sys.time()
}
index =  index + 1
if ((index %% 50) == 0){
percent_complete <- index / length(test_strings)
print(index / length(test_strings))
total_time <- (Sys.time() - start_time) / percent_complete
time_remaining <- total_time * (1 - percent_complete)
print(time_remaining)
}
result <- Predict(string,
model = imdb_pos_train_model,
ngram = 2,
gamma = 0.3) %>%
head(1)
result_words <- c(result_words, result[1, "word"])
result_P <- c(result_P, result[1, "P"])
}
test_imdb_pos_trigrams$predictions <- result_words
test_imdb_pos_trigrams$probability <- result_P
n <- Sys.time()
t <- n - t
print(t)
save(test_imdb_pos_trigrams, file = "output/test_imdb_pos_trigrams_5.RDa")
t <- Sys.time()
test_copy <- test_imdb_pos_quagrams
test_strings <- test_copy[["string"]]
result_words <- c()
result_P <- c()
index = 0
for (string in test_strings){
if (index == 0){
start_time <- Sys.time()
}
index =  index + 1
if ((index %% 50) == 0){
percent_complete <- index / length(test_strings)
print(index / length(test_strings))
total_time <- (Sys.time() - start_time) / percent_complete
time_remaining <- total_time * (1 - percent_complete)
print(time_remaining)
}
result <- Predict(string,
model = imdb_pos_train_model,
ngram = 3,
gamma = 0.3) %>%
head(1)
result_words <- c(result_words, result[1, "word"])
result_P <- c(result_P, result[1, "P"])
}
test_imdb_pos_quagrams$predictions <- result_words
test_imdb_pos_quagrams$probability <- result_P
n <- Sys.time()
t <- n - t
print(t)
save(test_imdb_pos_quagrams, file = "output/test_imdb_pos_quagrams_5.RDa")
base <- LoadSingleFile(
"data/ARTISTS/lyrics.txt",
SOS = TRUE, EOS = TRUE)
lyrics_unigrams <- CleanTokens(base, n = 1) %>%
WordFreq() %>%
filter(freq > 2)
lyrics_bigrams <- CleanTokens(base, n = 2) %>%
WordFreq() %>%
filter(freq > 1)
lyrics_trigrams <- CleanTokens(base, n = 3) %>%
WordFreq() %>%
filter(freq > 1)
lyrics_quagrams <- CleanTokens(base, n = 4) %>%
WordFreq() %>%
filter(freq > 1)
TrainTestSplit <- function(df, percent = 0.95, seed = 17373){
smp_size <- floor(percent * nrow(df))
set.seed(seed)
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
train <- df[train_ind, ]
test <- df[-train_ind, ]
results <- list(train = train, test = test)
}
trigrams_ <-TrainTestSplit(lyrics_trigrams, 0.95)
test_lyrics_trigrams <- trigrams_$test
train_lyrics_trigrams <- trigrams_$train
rm(trigrams_)
quagrams_ <-TrainTestSplit(lyrics_quagrams, 0.95)
test_lyrics_quagrams <- quagrams_$test
train_lyrics_quagrams <- quagrams_$train
rm(quagrams_)
lyrics_train_model <- CreateFreqTable(lyrics_unigrams,
lyrics_bigrams,
train_lyrics_trigrams,
train_lyrics_quagrams)
#save(lyrics_train_model, file = "models/lyrics_train_model.RDa")
RemoveTestDuplicates <- function(df, ngram){
if (ngram == 3){
df <- df %>%
separate(word, c("word1", "word2", "word3"), sep = " ") %>%
select(c("word1", "word2", "word3", "freq"))
df[, "word4"] <- NA
df <- distinct(df, word1, word2, .keep_all = TRUE)
} else if (ngram == 4){
df <- df %>%
separate(word, c("word1", "word2", "word3", "word4"), sep = " ") %>%
select(c("word1", "word2", "word3", "word4", "freq"))
df <- distinct(df, word1, word2, word3, .keep_all = TRUE)
}
df
}
test_lyrics_trigrams <- test_lyrics_trigrams %>%
RemoveTestDuplicates(ngram = 3) %>%
filter(!is.na(word1))
test_lyrics_quagrams <- test_lyrics_quagrams %>%
RemoveTestDuplicates(ngram = 4) %>%
filter(!is.na(word1))
CreatePredictionString <- function(df, ngram){
if (ngram == 3){
df <- df %>%
unite(col = string, word1, word2, sep = " ",
na.rm = TRUE, remove = FALSE)
} else if (ngram == 4){
df <- df %>%
unite(col = string, word1, word2, word3, sep = " ",
na.rm = TRUE, remove = FALSE)
}
}
test_lyrics_trigrams <- test_lyrics_trigrams %>%
CreatePredictionString(ngram = 3)
test_lyrics_quagrams <- test_lyrics_quagrams %>%
CreatePredictionString(ngram = 4)
t <- Sys.time()
test_copy <- test_lyrics_trigrams
test_strings <- test_copy[["string"]]
result_words <- c()
result_P <- c()
index = 0
for (string in test_strings){
if (index == 0){
start_time <- Sys.time()
}
index =  index + 1
if ((index %% 50) == 0){
percent_complete <- index / length(test_strings)
print(index / length(test_strings))
total_time <- (Sys.time() - start_time) / percent_complete
time_remaining <- total_time * (1 - percent_complete)
print(time_remaining)
}
result <- Predict(string,
model = lyrics_train_model,
ngram = 2,
gamma = 0.3) %>%
head(1)
result_words <- c(result_words, result[1, "word"])
result_P <- c(result_P, result[1, "P"])
}
test_lyrics_trigrams$predictions <- result_words
test_lyrics_trigrams$probability <- result_P
n <- Sys.time()
t <- n - t
print(t)
save(test_lyrics_trigrams, file = "output/test_lyrics_trigrams_5.RDa")
t <- Sys.time()
test_copy <- test_lyrics_quagrams
test_strings <- test_copy[["string"]]
result_words <- c()
result_P <- c()
index = 0
for (string in test_strings){
if (index == 0){
start_time <- Sys.time()
}
index =  index + 1
if ((index %% 50) == 0){
percent_complete <- index / length(test_strings)
print(index / length(test_strings))
total_time <- (Sys.time() - start_time) / percent_complete
time_remaining <- total_time * (1 - percent_complete)
print(time_remaining)
}
result <- Predict(string,
model = lyrics_train_model,
ngram = 3,
gamma = 0.3) %>%
head(1)
result_words <- c(result_words, result[1, "word"])
result_P <- c(result_P, result[1, "P"])
}
test_lyrics_quagrams$predictions <- result_words
test_lyrics_quagrams$probability <- result_P
n <- Sys.time()
t <- n - t
print(t)
save(test_lyrics_quagrams, file = "output/test_lyrics_quagrams_5.RDa")
twitter_tri_50 <- load("output/test_twitter_trigrams_50.Rda")
twitter_tri_25 <- load("output/test_twitter_trigrams_25.Rda")
twitter_tri_10 <- load("output/test_twitter_trigrams_10.Rda")
twitter_tri_5 <- load("output/test_twitter_trigrams_5.Rda")
twitter_qua_5 <- load("output/test_twitter_quagrams_5.Rda")
twitter_qua_10 <- load("output/test_twitter_quagrams_10.Rda")
twitter_qua_25 <- load("output/test_twitter_quagrams_25.Rda")
twitter_qua_50 <- load("output/test_twitter_quagrams_50.Rda")
test_twitter <- list(twitter_tri_5, twitter_tri_10, twitter_tri_25, twitter_tri_50, twitter_qua_5, twitter_qua_10, twitter_qua_25, twitter_qua_50)
test_twitter_tri <- test_twitter[1:4]
test_twitter_qua <- test_twitter[5:]
test_twitter_qua <- test_twitter[5:8]
lapply(test_twitter_tri, CostCheck, word3)
View(twitter_tri_50)
twitter_tri_50
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper2.R")
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper2.R")
twitter_qua_50 <- loadRData("output/test/test_twitter_quagrams_50.Rda")
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper2.R")
twitter_qua_50 <- loadRData("output/test/test_twitter_quagrams_50.Rda")
twitter_qua_5 <- loadRData("output/test/test_twitter_quagrams_5.Rda")
twitter_qua_25 <- loadRData("output/test/test_twitter_quagrams_25.Rda")
twitter_qua_10 <- loadRData("output/test/test_twitter_quagrams_10.Rda")
twitter_tri_5 <- loadRData("output/test/test_twitter_trigrams_5.Rda")
twitter_tri_15 <- loadRData("output/test/test_twitter_trigrams_25.Rda")
twitter_tri_50 <- loadRData("output/test/test_twitter_trigrams_50.Rda")
twitter_tri_10 <- loadRData("output/test/test_twitter_trigrams_10.Rda")
twitter_tri_25 <- twitter_tri_15
rm(twitter_tri_15)
twitter_tri <- list(twitter_tri_5, twitter_tri_10, twitter_tri_25, twitter_tri_50)
twitter_qua <- list(twitter_qua_5, twitter_qua_10, twitter_qua_25, twitter_qua_50)
lapply(twitter_tri, CostCheck, "word3")
lapply(twitter_qua, CostCheck, "word4")
blogs_tri <- loadRData("output/test/test_blogs_trigrams_5.RDa")
imdb_neg_tri <- loadRData("output/test/test_imdb_neg_trigrams_5.RDa")
imdb_pos_tri <- loadRData("output/test/test_imdb_pos_trigrams_5.RDa")
lyrics_tri <- loadRData("output/test/test_lyrics_trigrams_5.RDa")
news_tri <- loadRData("output/test/test_news_trigrams_5.RDa")
twitter_tri <- loadRData("output/test/test_twitter_trigrams_5.RDa")
blogs_qua <- loadRData("output/test/test_blogs_quagrams_5.RDa")
imdb_neg_qua <- loadRData("output/test/test_imdb_neg_quagrams_5.RDa")
imdb_pos_qua <- loadRData("output/test/test_imdb_pos_quagrams_5.RDa")
lyrics_qua <- loadRData("output/test/test_lyrics_quagrams_5.RDa")
news_qua <- loadRData("output/test/test_news_quagrams_5.RDa")
twitter_qua <- loadRData("output/test/test_twitter_quagrams_5.RDa")
test_tri <- list(blogs_tri, imdb_neg_tri, imdb_pos_tri, lyrics_tri, news_tri, twitter_tri)
test_qua <- list(blogs_qua, imdb_neg_qua, imdb_pos_qua, lyrics_qua, news_qua, twitter_qua)
lapply(test_tri, CostCheck, "word3")
lapply(test_qua, CostCheck, "word4")
