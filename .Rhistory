install.packages("tidytext")
library(tidytext)
install.packages(OpenNLP)
install.packages("OpenNLP")
install.packages("openNLP")
setwd("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing")
library(tidyverse)
library(tidytext)
library(ggplot2)
library(ggthemes)
library(tm)
library(SnowballC)
library(openNLP)
setwd("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing")
thou_twitter <- readLines("data/en_US/en_US.twitter.txt", 1000, skipNul = TRUE)
thou_news <- readLines("data/en_US/en_US.news.txt", 1000, skipNul = TRUE)
thou_blogs <- readLines("data/en_US/en_US.blogs.txt", 1000, skipNul = TRUE)
thou_twitter[[1]]
thou_twitter[1]
stemDocument(thou_twitter)
stemDocument(thou_twitter[1])
thou_news[1]
stemDocument(thou_news[1])
stemDocument(thou_news)
?tm_map
?tmMap
stripWhitespace(thou_twitter[1])
stripWhitespace(thou_twitter)
tmTolower(thou_twitter)
df_twitter <- tibble(thou_twitter)
df_news <- tibble(thou_news)
df_blogs <- tibble(thou_blogs)
View(df_blogs)
df_twitter %>% unnest_tokens(word, text)
df_twitter %>% unnest_tokens(word, thou_blogs)
df_twitter <- tibble(line = 1:1000, text = thou_twitter)
View(df_twitter)
df_twitter %>% unnest_tokens(word, text)
View(df_twitter)
data(stop_words)
df_twitter <- df_twitter %>% anti_join(stop_words)
?anti_join
df_twitter <- anti_join(df_twitter, stop_words)
df_twitter <- anti_join(df_twitter, stop_words, by = NULL)
View(stop_words)
View(df_twitter)
df_twitter <- df_tiwtter %>% unnest_tokens(word, text)
df_twitter <- df_twitter %>% unnest_tokens(word, text)
df_twitter <- anti_join(df_twitter, stop_words, by = NULL)
filter_words <- read.ftable("text_filter.txt", sep = "")
filter_words <- tibble(filter_words)
View(filter_words)
filter_words <- read.csv("text_filer.txt", sep="")
filter_words <- read.csv("text_filter.txt", sep="")
View(filter_words)
filter_words <- read.table("text_filter.txt", header=FALSE, sep="")
filter_words <- read.table("text_filter.txt", header=FALSE, sep="", col.names = "word")
df_twitter <- anti_join(df_twitter, filter_words)
df_twitter %>% count(word, sort=TRUE)
?join
data(stop_words)
filter_words <- read.table("text_filter.txt", header=FALSE, sep="", col.names = "word")
filter_words <- full_join(filter_words, stop_words)
thou_twitter <- readLines("data/en_US/en_US.twitter.txt", 1000, skipNul = TRUE)
thou_news <- readLines("data/en_US/en_US.news.txt", 1000, skipNul = TRUE)
thou_blogs <- readLines("data/en_US/en_US.blogs.txt", 1000, skipNul = TRUE)
df_twitter <- tibble(line = 1:1000, text = thou_twitter, book = "twitter")
df_twitter %>% unnest_tokens(word, text)
df_twitter <- anti_join(df_twitter, filter_words)
View(filter_words)
df_twitter <- unnest_tokens(df_twitter, word, text)
df_twitter <- anti_join(df_twitter, filter_words, by = "word")
# remove numbers
df_twitter <- filter(df_twitter, !grepl("\d+", word))
# remove numbers
df_twitter <- filter(df_twitter, !grepl("\\d+", word))
CleanFile <- function(file, bookname = "book", file_length = NULL){
if(is.null(file_length)){
file_length <- CheckLines(file)
}
df_ <- tibble(line = 1:file_length,
text = file,
book = bookname)
df_ <- unnest_tokens(df_, word, text)
df_ <- anti_join(df_, filter_words, by = "word")
df_ <- filter(df_, !grepl("\\d+", word))
df_
}
df_twitter <- tibble(line = 1:1000, text = thou_twitter, book = "twitter")
df_twitter <- unnest_tokens(df_twitter, word, text)
# remove filter words
df_twitter <- anti_join(df_twitter, filter_words, by = "word")
# remove numbers
df_twitter <- filter(df_twitter, !grepl("\\d+", word))
# check number of lines in a file
CheckLines <- function(file){
while (length(l <- readLines(file, 128, skipNul = TRUE)) >0 ){
nlines <- nlines + length(l)
}
nlines
}
# find max char length of a line in a file
LineMaxCharLength <- function(file){
lines <- CheckLines(file)
nchunk <- as.integer(lines / 1000) + 1
lengths <- c()
for (index in 1:nchunk){
temp <- c()
for (jndex in 1:1000){
temp <- c(temp, nchar(readLines(file, 1, skipNul = TRUE)))
}
lengths <- c(lengths, max(temp))
lengths
}
}
df_news <- CleanFile(thou_news, bookname = "news")
df_news <- CleanFile(thou_news, bookname = "news", file_length = 1000)
CleanFile <- function(file, bookname = "book", file_length = NULL){
text <- readLines(file, skipNul = TRUE)
if(is.null(file_length)){
file_length <- CheckLines(file)
}
df_ <- tibble(line = 1:file_length,
text = text,
book = bookname)
df_ <- unnest_tokens(df_, word, text)
df_ <- anti_join(df_, filter_words, by = "word")
df_ <- filter(df_, !grepl("\\d+", word))
df_
}
df_twitter <- tibble(line = 1:1000, text = thou_twitter, book = "twitter")
df_twitter <- unnest_tokens(df_twitter, word, text)
# remove filter words
df_twitter <- anti_join(df_twitter, filter_words, by = "word")
# remove numbers
df_twitter <- filter(df_twitter, !grepl("\\d+", word))
View(df_news)
df_blogs <- CleanFile(thou_blogs, bookname = "blogs", file_length = 1000)
CleanFile <- function(file, bookname = "book", file_length = NULL){
#text <- readLines(file, skipNul = TRUE)
if(is.null(file_length)){
file_length <- CheckLines(file)
}
df_ <- tibble(line = 1:file_length,
text = text,
book = bookname)
df_ <- unnest_tokens(df_, word, text)
df_ <- anti_join(df_, filter_words, by = "word")
df_ <- filter(df_, !grepl("\\d+", word))
df_
}
df_twitter <- tibble(line = 1:1000, text = thou_twitter, book = "twitter")
df_twitter <- unnest_tokens(df_twitter, word, text)
# remove filter words
df_twitter <- anti_join(df_twitter, filter_words, by = "word")
# remove numbers
df_twitter <- filter(df_twitter, !grepl("\\d+", word))
df_blogs <- CleanFile(thou_blogs, bookname = "blogs", file_length = 1000)
CleanFile <- function(file, bookname = "book", file_length = NULL){
if(is.null(file_length)){
file_length <- CheckLines(file)
}
df_ <- tibble(line = 1:file_length,
text = file(),
book = bookname)
df_ <- unnest_tokens(df_, word, text)
df_ <- anti_join(df_, filter_words, by = "word")
df_ <- filter(df_, !grepl("\\d+", word))
df_
}
df_blogs <- CleanFile(thou_blogs, bookname = "blogs", file_length = 1000)
thou_blogs
df_twitter <- CleanFile(thou_twitter, bookname = "twitter", file_length = 1000)
CleanFile <- function(file, bookname = "book", file_length = NULL){
if(is.null(file_length)){
file_length <- CheckLines(file)
}
df_ <- tibble(line = 1:file_length,
text = file,
book = bookname)
df_ <- unnest_tokens(df_, word, text)
df_ <- anti_join(df_, filter_words, by = "word")
df_ <- filter(df_, !grepl("\\d+", word))
df_
}
df_twitter <- CleanFile(thou_twitter, bookname = "twitter", file_length = 1000)
df_twitter <- CleanFile(thou_twitter, bookname = "twitter", file_length = 1000)
df_news <- CleanFile(thou_news, bookname = "news", file_length = 1000)
df_blogs <- CleanFile(thou_blogs, bookname = "blogs", file_length = 1000)
?merge
df <- merge(df_twitter, df_news) %>% merge(df_blogs)
View(df)
df <- rbind(df_twitter, df_news) %>% rbind(df_blogs)
View(df)
freq_by_rank <- df %>% group_by(book) %>% mutate(rank = row_number(), "term frequency" = n/total) %>% ungroup()
df_words <- df %>% count(book, word, sort=TRUE)
View(df_words)
df_total <- df_words %>% group_by(book) %>% summarise(total = sum(n))
View(df_total)
df_book <- left_join(df_words, df_total)
View(df_book)
freq_by_rank <- df %>% group_by(book) %>% mutate(rank = row_number(), "term frequency" = n/total) %>% ungroup()
freq_by_rank <- df_book %>% group_by(book) %>% mutate(rank = row_number(), "term frequency" = n/total) %>% ungroup()
View(freq_by_rank)
freq_by_rank %>%
ggplot(aes(rank, `term frequency`, color = book)) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
scale_x_log10() +
scale_y_log10()
freq_by_rank %>%
ggplot(aes(rank, `term frequency`, color = book)) +
geom_abline(intercept = -0.62, slope = -1.1,
color = "gray50", linetype = 2) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
scale_x_log10() +
scale_y_log10()
book_tf_idf <- df_book %>%
bind_tf_idf(word, book, n)
book_tf_idf %>%
select(-total) %>%
arrange(desc(tf_idf))
library(forcats)
book_tf_idf %>%
group_by(book) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = book)) +
geom_col(show.legend = FALSE) +
facet_wrap(~book, ncol = 2, scales = "free") +
labs(x = "tf-idf", y = NULL)
