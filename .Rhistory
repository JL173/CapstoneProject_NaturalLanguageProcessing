# working directory
setwd("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing")
# modules
library(tidyverse)
library(tidytext)
library(tm)
library(openNLP)
library(data.table)
library(parallel)
library(doParallel)
base_chunks <- UnlistDF(LoadFiles("data/en_US/"), size = 1e5)
chunk_number = 150
base_chunks_chunks <- split(base_chunks,
cut(seq_along(base_chunks),
chunk_number,
labels = FALSE))
rm(base_chunks)
POS_summary_list <- list()
for (index in 1:length(base_chunks_chunks)){
gc()
options(java.parameters = "-Xmx16000m")
temp_list_ <- lapply(base_chunks_chunks[[index]], POSsumDF)
POS_summary_list <- c(POS_summary_list, MergeDTM(temp_list_))
rm(temp_list_)
print(index)
}
base_chunks <- UnlistDF(LoadFiles("data/en_US/"), size = 1e5)
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
base_chunks <- UnlistDF(LoadFiles("data/en_US/"), size = 1e5)
chunk_number = 150
base_chunks_chunks <- split(base_chunks,
cut(seq_along(base_chunks),
chunk_number,
labels = FALSE))
rm(base_chunks)
POS_summary_list <- list()
for (index in 1:length(base_chunks_chunks)){
gc()
options(java.parameters = "-Xmx16000m")
temp_list_ <- lapply(base_chunks_chunks[[index]], POSsumDF)
POS_summary_list <- c(POS_summary_list, MergeDTM(temp_list_))
rm(temp_list_)
print(index)
}
save(POS_summary_list, "POS_summary_list.Rda")
?save
save(POS_summary_list)
save(POS_summary_list, file = "POS_summary_list.Rda")
View(POS_summary_list)
POS_summary_df <- MergeDTM(POS_summary_list)
View(POS_summary_df)
POS_summary_df[1]
POS_summary_list[1]
POS_summary_list[2]
POS_summary_list[3]
data.table(document = POS_summary_list[1],
POS = POS_summary_list[2],
total = POS_summary_list[3])
df_test <- data.table(document = POS_summary_list[1],
POS = POS_summary_list[2],
total = POS_summary_list[3])
View(df_test)
df_test <- data.table(document = POS_summary_list[1],
POS = POS_summary_list[2],
total = POS_summary_list[[3]])
View(df_test)
df_test <- data.table(document = POS_summary_list[[1]],
POS = POS_summary_list[[2]],
total = POS_summary_list[[3]])
# combine the POS_summary_list object into one dataframe
MergeList <- function(list){
length <- length(list)/3 - 1
df_ <- data.table(document = list[[1]],
POS = list[[2]],
total = list[[3]])
for (index in 2:length){
temp_df <- data.table(document = list[[3*index]],
POS = list[[3*index+1]],
total = list[[3*index+2]])
df_ <- rbind(df_, temp_df)
}
df_
}
df_test <- MergeList(POS_summary_list)
View(df_test)
df_test <- MergeList(POS_summary_list)
View(df_test)
# combine the POS_summary_list object into one dataframe
MergeList <- function(list){
length <- length(list)/3 - 1
df_ <- data.table(document = list[[1]],
POS = list[[2]],
total = list[[3]])
for (index in 1:length){
temp_df <- data.table(document = list[[3*index+1]],
POS = list[[3*index+2]],
total = list[[3*index+3]])
df_ <- rbind(df_, temp_df)
}
df_
}
df_test <- MergeList(POS_summary_list)
View(df_test)
save(df_test, file = "POS_summary_df.Rda")
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
POS_summary_df %>% group_by(document) %>% slice_max(n, n = 15) %>%
ungroup() %>%
arrange(n) %>%
ggplot(aes(x = n, y = fct_reorder(POS, n), fill = document)) +
geom_col() +
facet_wrap(~document, scales = 'free') +
labs(title = "Most frequent POS per document", y = NULL, x = "Frequency")
df_test %>% group_by(document) %>% slice_max(n, n = 15) %>%
ungroup() %>%
arrange(n) %>%
ggplot(aes(x = n, y = fct_reorder(POS, n), fill = document)) +
geom_col() +
facet_wrap(~document, scales = 'free') +
labs(title = "Most frequent POS per document", y = NULL, x = "Frequency")
View(df_test)
df_test %>% group_by(document) %>% slice_max(n, n = 15) %>% head()
df_test %>% group_by(document) %>% slice_max(total, n = 15) %>% head()
df_test %>% group_by(document) %>% slice_max(total, n = 15) %>%
ungroup() %>%
arrange(n) %>%
ggplot(aes(x = total, y = fct_reorder(POS, total), fill = document)) +
geom_col() +
facet_wrap(~document, scales = 'free') +
labs(title = "Most frequent POS per document", y = NULL, x = "Frequency")
df_test %>% group_by(document) %>% slice_max(total, n = 15) %>%
ungroup() %>%
arrange(total) %>%
ggplot(aes(x = total, y = fct_reorder(POS, total), fill = document)) +
geom_col() +
facet_wrap(~document, scales = 'free') +
labs(title = "Most frequent POS per document", y = NULL, x = "Frequency")
df_test %>% group_by(document) %>%
arrange(total) %>%
ggplot(aes(x = total, y = fct_reorder(POS, total), fill = document)) +
geom_col() +
facet_wrap(~document, scales = 'free') +
labs(title = "Most frequent POS per document", y = NULL, x = "Frequency")
df_test %>% group_by(document) %>%
arrange(total) %>%
ggplot(aes(x = total, y = fct_reorder(POS, total), fill = document)) +
geom_col() +
facet_wrap(~document, scales = 'fixed') +
labs(title = "Most frequent POS per document", y = NULL, x = "Frequency")
df_test %>% group_by(document) %>%
arrange(total) %>%
ggplot(aes(x = total, y = fct_reorder(POS, total), fill = document)) +
geom_col() +
facet_wrap(~document, scales = 'fixed') +
labs(title = "Most frequent POS per document", y = NULL, x = "Frequency")
df_test %>% group_by(document) %>%
arrange(total) %>%
ggplot(aes(x = total, y = fct_reorder(POS, total), fill = document)) +
geom_col() + scale_x_log10() +
facet_wrap(~document, scales = 'fixed') +
labs(title = "Most frequent POS per document", y = NULL, x = "Frequency")
df_test2 <- df_test %>% group_by(document) %>% mutate(PFreq = total/n(document))
df_test2 <- df_test %>% group_by(document) %>% summarise(count = n(document))
df_test2 <- df_test %>% group_by(document) %>% summarise(count = count(document))
df_test2 <- df_test %>% group_by(document) %>% summarise(count = n())
head(df_test2)
df_test2 <- df_test %>% group_by(document) %>% mutate(PFreq = total/n())
head(df_test2)
df_test2 %>% group_by(document) %>%
arrange(PFreq) %>%
ggplot(aes(x = PFreq, y = fct_reorder(POS, PFreq), fill = document)) +
geom_col() +
facet_wrap(~document, scales = 'fixed') +
labs(title = "Most frequent POS per document", y = NULL, x = "Frequency")
df_test %>% group_by(document) %>%
mutate(PFreq = total/n()) %>%
arrange(PFreq) %>%
ggplot(aes(x = PFreq, y = fct_reorder(POS, PFreq), fill = document)) +
geom_col() +
facet_wrap(~document, scales = 'fixed') +
labs(title = "Most frequent POS per document", y = NULL, x = "Frequency")
load("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/POS_summary_df.Rda")
POS_summary_df <- df_test
save(POS_summary_df, file = "POS_summary_df.Rda")
full_tidy_dtm_3 %>% group_by(document) %>% slice_max(n, n = 15) %>%
ungroup() %>%
arrange(n) %>%
ggplot(aes(x = n, y = fct_reorder(word, n), fill = document)) +
geom_col() +
facet_wrap(~document, scales = 'free') +
labs(title = "Most frequent Trigrams per document", y = NULL, x = "Frequency")
# working directory
setwd("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing")
# modules
library(tidyverse)
library(tidytext)
library(tm)
library(openNLP)
library(data.table)
library(parallel)
library(doParallel)
# helper functions
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
# Load tidy dtm
load("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/full_tidy_dtm_ALL.RData")
setDT(full_tidy_dtm_1)
setDT(full_tidy_dtm_2)
setDT(full_tidy_dtm_3)
# Load POS_summary_df dataframe
load("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/POS_summary_df.Rda")
full_tidy_dtm_3 %>% group_by(document) %>% slice_max(n, n = 15) %>%
ungroup() %>%
arrange(n) %>%
ggplot(aes(x = n, y = fct_reorder(word, n), fill = document)) +
geom_col() +
facet_wrap(~document, scales = 'free') +
labs(title = "Most frequent Trigrams per document", y = NULL, x = "Frequency")
full_tidy_dtm_2 %>% group_by(document) %>% slice_max(n, n = 15) %>%
ungroup() %>%
arrange(n) %>%
ggplot(aes(x = n, y = fct_reorder(word, n), fill = document)) +
geom_col() +
facet_wrap(~document, scales = 'free') +
labs(title = "Most frequent Bigrams per document", y = NULL, x = "Frequency")
