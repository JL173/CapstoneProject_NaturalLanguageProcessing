# working directory
setwd("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing")
# modules
library(tidyverse)
library(tidytext)
library(tm)
library(openNLP)
library(data.table)
library(parallel)
library(doParallel)
# helper functions
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
# Load tidy dtm
load("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/full_tidy_dtm_ALL.RData")
setDT(full_tidy_dtm_1)
setDT(full_tidy_dtm_2)
setDT(full_tidy_dtm_3)
# Load POS_summary_df dataframe
load("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/POS_summary_df.Rda")
POS_summary_df %>% group_by(document) %>%
mutate(PFreq = total/n()) %>%
arrange(PFreq) %>%
ggplot(aes(x = PFreq, y = fct_reorder(POS, PFreq), fill = document)) +
geom_col() +
facet_wrap(~document, scales = 'fixed') +
labs(title = "Most frequent POS per line per document", y = NULL, x = "Frequency")
# calculating probabilities of 'next word'
base_1000 <- LoadFiles("data/en_US/", n = 1000)
tidy_1 <- lapply(base_1000, CleanTokens, n = 1) %>% MergeDTM()
tidy_2 <- lapply(base_1000, CleanTokens, n = 2) %>% MergeDTM()
tidy_3 <- lapply(base_1000, CleanTokens, n = 3) %>% MergeDTM()
unigrams <- WordFreqProb(tidy_1, n = 2000)
bigrams <- WordFreqProb(tidy_2)
trigrams <- WordFreqProb(tidy_3)
unigram_probs <- unigrams
bigram_probs <- bigrams %>%
left_join(unigrams, by = c("word1" = "word"), suffix = c(".x", ".y")) %>%
mutate(P = freq.x / freq.y) %>%
drop_na() %>%
select(-c("p.x", "p.y")) %>%
arrange(word1, desc(P))
View(bigrams)
View(unigrams)
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
# calculating probabilities of 'next word'
base_1000 <- LoadFiles("data/en_US/", n = 1000)
tidy_1 <- lapply(base_1000, CleanTokens, n = 1) %>% MergeDTM()
tidy_2 <- lapply(base_1000, CleanTokens, n = 2) %>% MergeDTM()
tidy_3 <- lapply(base_1000, CleanTokens, n = 3) %>% MergeDTM()
unigrams <- WordFreqProb(tidy_1, n = 2000)
# calculating probabilities of 'next word'
base_1000 <- LoadFiles("data/en_US/", n = 1000)
tidy_1 <- lapply(base_1000, CleanTokens, n = 1) %>% MergeDTM()
tidy_2 <- lapply(base_1000, CleanTokens, n = 2) %>% MergeDTM()
tidy_3 <- lapply(base_1000, CleanTokens, n = 3) %>% MergeDTM()
unigrams <- WordFreqProb(tidy_1, k = 2000)
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
# calculating probabilities of 'next word'
base_1000 <- LoadFiles("data/en_US/", n = 1000)
tidy_1 <- lapply(base_1000, CleanTokens, n = 1) %>% MergeDTM()
tidy_2 <- lapply(base_1000, CleanTokens, n = 2) %>% MergeDTM()
tidy_3 <- lapply(base_1000, CleanTokens, n = 3) %>% MergeDTM()
unigrams <- WordFreqProb(tidy_1, k = 2000)
bigrams <- WordFreqProb(tidy_2)
View(unigrams)
View(bigrams)
View(tidy_2)
View(bigrams)
bigrams <- WordFreqProb(tidy_2)
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
bigrams <- WordFreqProb(tidy_2)
View(bigrams)
# calculating probabilities of 'next word'
base_1000 <- LoadFiles("data/en_US/", n = 1000)
tidy_1 <- lapply(base_1000, CleanTokens, n = 1) %>% MergeDTM()
tidy_2 <- lapply(base_1000, CleanTokens, n = 2) %>% MergeDTM()
tidy_3 <- lapply(base_1000, CleanTokens, n = 3) %>% MergeDTM()
unigrams <- WordFreqProb(tidy_1, k = 2000)
bigrams <- WordFreqProb(tidy_2)
trigrams <- WordFreqProb(tidy_3)
unigram_probs <- unigrams
bigram_probs <- bigrams %>%
left_join(unigrams, by = c("word1" = "word"), suffix = c(".x", ".y")) %>%
mutate(P = freq.x / freq.y) %>%
drop_na() %>%
select(-c("p.x", "p.y")) %>%
arrange(word1, desc(P))
View(bigrams)
# calculating probabilities of 'next word'
base_1000 <- LoadFiles("data/en_US/", n = 1000)
tidy_1 <- lapply(base_1000, CleanTokens, n = 1) %>% MergeDTM()
tidy_2 <- lapply(base_1000, CleanTokens, n = 2) %>% MergeDTM()
tidy_3 <- lapply(base_1000, CleanTokens, n = 3) %>% MergeDTM()
unigrams <- WordFreqProb(tidy_1, k = 2000)
bigrams <- WordFreqProb(tidy_2)
trigrams <- WordFreqProb(tidy_3)
unigram_probs <- unigrams
bigram_probs <- bigrams %>%
separate(word, c("word1", "word2"), sep = " ") %>%
left_join(unigrams, by = c("word1" = "word"), suffix = c(".x", ".y")) %>%
mutate(P = freq.x / freq.y) %>%
drop_na() %>%
select(-c("p.x", "p.y")) %>%
arrange(word1, desc(P))
trigram_probs <- trigrams %>%
separate(word, c("word1", "word2", "word3"), sep = " ") %>%
unite(col = word, word1, word2, sep = " ", na.rm = TRUE) %>%
left_join(unite(bigrams, col = word, word1, word2, sep = " "), by = c("word" = "word"), suffix = c(".x", ".y")) %>%
drop_na() %>%
mutate(P = freq.x / freq.y) %>%
select(-c("p.x", "p.y")) %>%
arrange(word, desc(P))
View(trigrams)
trigrams %>%
separate(word, c("word1", "word2", "word3"), sep = " ") %>% head()
# calculating probabilities of 'next word'
base_1000 <- LoadFiles("data/en_US/", n = 1000)
tidy_1 <- lapply(base_1000, CleanTokens, n = 1) %>% MergeDTM()
tidy_2 <- lapply(base_1000, CleanTokens, n = 2) %>% MergeDTM()
tidy_3 <- lapply(base_1000, CleanTokens, n = 3) %>% MergeDTM()
unigrams <- WordFreqProb(tidy_1, k = 2000)
bigrams <- WordFreqProb(tidy_2)
trigrams <- WordFreqProb(tidy_3)
unigram_probs <- unigrams
bigram_probs <- bigrams %>%
separate(word, c("word1", "word2"), sep = " ") %>%
left_join(unigrams, by = c("word1" = "word"), suffix = c(".x", ".y")) %>%
mutate(P = freq.x / freq.y) %>%
drop_na() %>%
select(-c("p.x", "p.y")) %>%
arrange(word1, desc(P))
trigram_probs <- trigrams %>%
separate(word, c("word1", "word2", "word3"), sep = " ") %>%
unite(col = word, word1, word2, sep = " ", na.rm = TRUE) %>%
left_join(bigrams, by = c("word" = "word"), suffix = c(".x", ".y")) %>%
drop_na() %>%
mutate(P = freq.x / freq.y) %>%
select(-c("p.x", "p.y")) %>%
arrange(word, desc(P))
View(trigram_probs)
object.size(base_1000)
object.size(base_1000)/1e6
object.size(base_1000)/1e5
object.size(unigrams)/1e5
object.size(unigrams)/1e6
object.size(tidy_3)/1e6
object.size(tidy_2)/1e6
object.size(tidy_1)/1e6
LoadFiles("data/en_US/", n = c(1, 10))
LoadFiles("data/en_US/", n = 1:10)
LoadFiles("data/en_US/", n = 1)
LoadFiles("data/en_US/", n = 10)
LoadFiles("data/en_US/", n = c(1, 10))
doc <- readLines("data/en_US/en_US.Twitter.txt", n = 1, skipNul = TRUE, encoding = "UTF-8")
doc
doc <- readLines("data/en_US/en_US.Twitter.txt", n = 2, skipNul = TRUE, encoding = "UTF-8")
doc
docstem <- stemDocument(doc)
docstem
docstem[1]
docstem[2]
doctest <- c(docstem[1], docstem[2])
doctest
doc <- readLines("data/en_US/en_US.Twitter.txt", n = 2, skipNul = TRUE, encoding = "UTF-8")
docr <- read_lines("data/en_US/en_US.Twitter.txt", skip = 0, skip_empty_rows = TRUE, n_max = 2, locale = "UTF-8")
docr <- read_lines_raw("data/en_US/en_US.Twitter.txt", skip = 0, n_max = 2)
docr
docr <- read_lines("data/en_US/en_US.Twitter.txt", skip = 0, skip_empty_rows = TRUE, n_max = 2, locale = "UTF-8")
locale()
docr <- read_lines("data/en_US/en_US.Twitter.txt", skip = 0, skip_empty_rows = TRUE, n_max = 2)
docr
doc <- readLines("data/en_US/en_US.Twitter.txt", n = 2, skipNul = TRUE, encoding = "UTF-8")
docr
doc
binomial()
?binomial
rbinom
?rbinom
rbinom(10, 1, 0.1)
rbinom(1, 1, 0.1)
rbinom(10, 1, 0.1)
rbinom(10, 5, 0.1)
rbinom(10, 15, 0.1)
?runif
runif(10, min = 0, max = 100)
as.integer(runif(10, min = 0, max = 100))
as.integer(runif(10, min = 1, max = 100))
# check number of lines in a file
CheckLines <- function(file){
while (length(l <- readLines(file, 128, skipNul = TRUE)) >0 ){
nlines <- nlines + length(l)
}
nlines
}
# LoadFiles with sampling
SampleLoadFiles <- function(directory, pattern = "*.txt", n = 1000){
filenames <- list.files(directory, pattern = pattern, full.names = TRUE)
shortnames <- list.files(directory, pattern = pattern, full.names = FALSE)
document_list <- list()
for(index in 1:length(filenames)){
#find length of document
nlines <- CheckLines(filenames[index])
#generate sample indicies
sample <- as.integer(runif(n, min = 1, max = nlines))
document <- c()
#sample reading
for (jndex in 1:n){
#reading
document_line <- read_lines(filenames[index],
skip = jndex-1,
skip_empty_rows = TRUE,
n_max = 1,
locale = default_locale())
#stemming
document_line <- stemDocument(document_line)
document <- c(document, document_line)
}
#dataframe
df_ <- tibble(line = 1:length(document),
text = document,
document = shortnames[index])
#data.table format
setDF(df_)
# list of dataframes
document_list[[index]] <- df_
}
document_list
}
source("~/.active-rstudio-document", echo=TRUE)
base_10 <- LoadFiles("data/en_US/", n =10)
base_10
sample_10 <- SampleLoadFiles("data/en_US/", n =10)
source("~/.active-rstudio-document", echo=TRUE)
sample_10 <- SampleLoadFiles("data/en_US/", n =10)
source("~/.active-rstudio-document", echo=TRUE)
sample_10 <- SampleLoadFiles("data/en_US/", n =10)
CheckLines("data/en_US/en_US.Twitter.txt")
as.integer(runif(10, min = 1, max = 2400000))
as.integer(runif(10, min = 1, max = 2400000))
as.integer(runif(10, min = 1, max = 2400000))
as.integer(runif(10, min = 1, max = 2400000))
CheckLines("data/en_US/en_US.Twitter.txt")
sample_10 <- SampleLoadFiles("data/en_US/", n =10)
source("~/.active-rstudio-document", echo=TRUE)
as.integer(runif(10, min = 1, max = 2400000))
CheckLines("data/en_US/en_US.Twitter.txt")
as.integer(runif(10, min = 1, max = 2400000))
as.integer(runif(10, min = 1, max = 2400000))
as.integer(runif(10, min = 1, max = 2400000))
as.integer(runif(10, min = 1, max = 2400000))
as.integer(runif(10, min = 1, max = 2400000))
as.integer(runif(10, min = 1, max = 2400000))
as.integer(runif(10, min = 1, max = 2400000))
as.integer(runif(10, min = 1, max = 2400000))
as.integer(runif(10, min = 1, max = 2400000))
as.integer(runif(10, min = 1, max = 80000))
as.integer(runif(10, min = 1, max = 80000))
as.integer(runif(10, min = 1, max = 80000))
as.integer(runif(10, min = 1, max = 80000))
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
sample_10 <- SampleLoadFiles("data/en_US/", n =10)
?countLines
library(R.utils)
install.packages("R.utils")
library(R.utils)
source("~/.active-rstudio-document", echo=TRUE)
sample_10 <- SampleLoadFiles("data/en_US/", n =10)
source("~/.active-rstudio-document", echo=TRUE)
sample_10 <- SampleLoadFiles("data/en_US/", n =10)
source("~/.active-rstudio-document", echo=TRUE)
sample_10 <- SampleLoadFiles("data/en_US/", n =10)
sample_10
source("~/.active-rstudio-document", echo=TRUE)
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
