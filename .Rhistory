# coursera quiz predictions
test_strings2 <- c("When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd", "Guy at my table's wife got up to go to the bathroom and I asked about dessert and he started telling me about his", "I'd give anything to see arctic monkeys this", "Talking to your mom has the same effect as a hug and helps reduce your", "When you were in Holland you were like 1 inch away from me but you hadn't time to take a", "I'd just like all of these questions answered, a presentation of evidence, and a jury to settle the", "I can't deal with unsymetrical things. I can't even hold an uneven number of bags of groceries in each", "Every inch of you is perfect from the bottom to the", "Iâ€™m thankful my childhood was filled with imagination and bruises from playing", "I like how the same people are in almost all of Adam Sandler's")
setwd("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing")
library(tidyverse)
library(tidytext)
library(tm)
library(ggthemes)
library(openNLP)
library(R.utils)
library(data.table)
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper2.R")
load("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/full_unigram.RDa")
unigram_ <- unigram_ %>% filter(freq > 1)
gc()
load("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/full_bigram.RDa")
bigram_ <- bigram_ %>% filter(freq > 1)
gc()
load("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/full_trigram.RDa")
trigram_ <- trigram_ %>% filter(freq > 1)
gc()
load("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/full_quagram.RDa")
quagram_ <- quagram_ %>% filter(freq > 1)
gc()
full_model <- CreateFreqTable(unigram_, bigram_,
trigram_, quagram_)
View(full_model)
for (string in test_strings2){
results <- Predict(string, full_model, ngram = 3, gamma = 0.3) %>% head(100)
print(results)
}
pos <- LoadFiles("data/en_us/IMDB/pos")
neg <- LoadFiles("data/en_US/IMDB/neg")
imdb_test <- LoadFiles("data/en_us/IMDB")
imdb <- c(pos, neg)
View(imdb)
t <- Sys.time()
imdb_unigrams <- lapply(imdb, CleanTokens, n = 1) %>%
MergeDTM() %>%
WordFreq() %>%
filter(freq > 1)
imdb <- LoadFiles("data/en_us/IMDB")
t <- Sys.time()
imdb_unigrams <- lapply(imdb, CleanTokens, n = 1) %>%
MergeDTM() %>%
WordFreq() %>%
filter(freq > 1)
n <- Sys.time() - t
print(n)
View(imdb_unigrams)
t <- Sys.time()
imdb_unigrams <- lapply(imdb, CleanTokens, n = 2) %>%
MergeDTM() %>%
WordFreq() %>%
filter(freq > 1)
n <- Sys.time() - t
print(n)
t <- Sys.time()
imdb_unigrams <- lapply(imdb, CleanTokens, n = 3) %>%
MergeDTM() %>%
WordFreq() %>%
filter(freq > 1)
n <- Sys.time() - t
print(n)
t <- Sys.time()
imdb <- LoadFiles("data/en_us/IMDB")
n <- Sys.time() - t
print(n)
t <- Sys.time()
imdb_unigrams <- lapply(imdb, CleanTokens, n = 1) %>%
MergeDTM() %>%
WordFreq() %>%
filter(freq > 1)
n <- Sys.time() - t
print(n)
t <- Sys.time()
imdb_unigrams <- lapply(imdb, CleanTokens, n = 2) %>%
MergeDTM() %>%
WordFreq() %>%
filter(freq > 1)
n <- Sys.time() - t
print(n)
t <- Sys.time()
imdb_unigrams <- lapply(imdb, CleanTokens, n = 3) %>%
MergeDTM() %>%
WordFreq() %>%
filter(freq > 1)
n <- Sys.time() - t
print(n)
t <- Sys.time()
imdb_unigrams <- lapply(imdb, CleanTokens, n = 4) %>%
MergeDTM() %>%
WordFreq() %>%
filter(freq > 1)
n <- Sys.time() - t
print(n)
t <- Sys.time()
imdb_model <- CreateFreqTable(imdb_unigrams,
imdb_bigrams,
imdb_trigrams,
imdb_quagrams)
t <- Sys.time()
imdb_bigrams <- lapply(imdb, CleanTokens, n = 2) %>%
MergeDTM() %>%
WordFreq() %>%
filter(freq > 1)
n <- Sys.time() - t
print(n)
t <- Sys.time()
imdb_trigrams <- lapply(imdb, CleanTokens, n = 3) %>%
MergeDTM() %>%
WordFreq() %>%
filter(freq > 1)
n <- Sys.time() - t
print(n)
t <- Sys.time()
imdb_quagrams <- lapply(imdb, CleanTokens, n = 4) %>%
MergeDTM() %>%
WordFreq() %>%
filter(freq > 1)
n <- Sys.time() - t
print(n)
t <- Sys.time()
imdb_model <- CreateFreqTable(imdb_unigrams,
imdb_bigrams,
imdb_trigrams,
imdb_quagrams)
n <- Sys.time() - t
print(n)
for (string in test_strings2){
results <- Predict(string, imdb_model, ngram = 3, gamma = 0.3) %>% head(20)
print(results)
}
for (string in test_strings2){
results <- Predict(string, imdb_model, ngram = 3, gamma = 0.3) %>% head(20)
print(results)
}
View(imdb_model)
t <- Sys.time()
imdb_unigrams <- lapply(imdb, CleanTokens, n = 1) %>%
MergeDTM() %>%
WordFreq() %>%
filter(freq > 1)
n <- Sys.time() - t
print(n)
t <- Sys.time()
imdb_model <- CreateFreqTable(imdb_unigrams,
imdb_bigrams,
imdb_trigrams,
imdb_quagrams)
n <- Sys.time() - t
print(n)
for (string in test_strings2){
results <- Predict(string, imdb_model, ngram = 3, gamma = 0.3) %>% head(20)
print(results)
}
