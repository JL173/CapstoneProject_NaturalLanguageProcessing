setwd("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing")
library(tidyverse)
library(tidytext)
library(tm)
library(openNLP)
library(data.table)
# helper functions
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
# Load tidy dtm
load("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/full_tidy_dtm_ALL.RData")
setDT(full_tidy_dtm_1)
setDT(full_tidy_dtm_2)
setDT(full_tidy_dtm_3)
class(full_tidy_dtm_1)
base_files <- LoadFiles("data/en_US/")
base_chunks <- UnlistDF(base_files)
View(base_chunks)
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
base_files <- LoadFiles("data/en_US/")
base_chunks <- UnlistDF(base_files)
View(base_files)
View(base_chunks)
base_chunks <- UnlistDF(base_files, size = 5e5)
View(base_chunks)
detectCores()
detectCores()
library(parallel)
detectCores
detectCores()
POSDF <- function(df){
options(java.parameters = "-Xmx8000m")
df["text"] <- mclapply(df["text"], POSstring, mc.cores = 8)
df
}
POSDF(base_chunks[[1]])
POSDF <- function(df){
options(java.parameters = "-Xmx8000m")
df["text"] <- mclapply(df["text"], POSstring, mc.cores = 1)
df
}
POSDF(base_chunks[[1]])
library(snow)
install.packages("snow")
library(snow)
POSDFsnow <- function(df){
options(java.parameters = "-Xmx8000m")
cl <- makeCluster(8)
df["text"] <- parLapply(df["text"], POSstring)
stopCluster()
df
}
system.time(POSDF(base_chunks[[1]]))
system.time(POSDFsnow(base_chunks[[1]]))
POSDFsnow <- function(df){
options(java.parameters = "-Xmx8000m")
cl <- makeCluster(8, type = "SOCK")
df["text"] <- parLapply(df["text"], POSstring)
stopCluster()
df
}
system.time(POSDFsnow(base_chunks[[1]]))
library(doParallel)
POSDFsnow <- function(df){
options(java.parameters = "-Xmx8000m")
cl <- makePSOCKCluster(8)
registerDoParallel(cl)
df["text"] <- parLapply(df["text"], POSstring)
stopCluster(cl)
df
}
POSDFdo <- function(df){
options(java.parameters = "-Xmx8000m")
cl <- makePSOCKCluster(8)
registerDoParallel(cl)
df["text"] <- parLapply(df["text"], POSstring)
stopCluster(cl)
df
}
system.time(POSDFdo(base_chunks[[1]]))
POSDFdo <- function(df){
options(java.parameters = "-Xmx8000m")
cl <- makePSOCKCluster(8)
registerDoParallel(cl)
df["text"] <- mclapply(df["text"], POSstring)
stopCluster(cl)
df
}
system.time(POSDFdo(base_chunks[[1]]))
POSDFdo <- function(df){
options(java.parameters = "-Xmx8000m")
cl <- makePSOCKcluster(8)
registerDoParallel(cl)
df["text"] <- mclapply(df["text"], POSstring)
stopCluster(cl)
df
}
system.time(POSDFdo(base_chunks[[1]]))
POSDFsnow <- function(df){
options(java.parameters = "-Xmx8000m")
cl <- makeCluster(8, type = "SOCK")
df["text"] <- parLapply(df["text"], POSstring)
stopCluster(cl)
df
}
POSDFsnow <- function(df){
options(java.parameters = "-Xmx8000m")
cl <- makeCluster(8, type = "SOCK")
df["text"] <- clusterApply(df["text"], POSstring)
stopCluster(cl)
df
}
system.time(POSDFsnow(base_chunks[[1]]))
POSDFsnow <- function(df){
options(java.parameters = "-Xmx8000m")
cl <- makeCluster(8, type = "SOCK")
df["text"] <- clusterApply(cl, df["text"], POSstring)
stopCluster(cl)
df
}
system.time(POSDFsnow(base_chunks[[1]]))
library(tidyverse)
library(tidytext)
library(tm)
library(openNLP)
library(data.table)
library(parallel)
library(doParallel)
library(snow)
system.time(POSDFsnow(base_chunks[[1]]))
POSDFparallel <- function(df){
options(java.parameters = "-Xmx8000m")
registerDoParallel(cores = 8)
cl <- makeCluster(8, type = "SOCK")
df["text"] <- parLapply(cl, df["text"], POSstring)
stopCluster(cl)
df
}
system.time(POSDFparallel(base_chunks[[1]]))
ls()
?rm
lsf.str()
>clusterEvalQ
?clusterEvalQ
POSDFparallel <- function(df){
options(java.parameters = "-Xmx8000m")
registerDoParallel(cores = 8)
cl <- makeCluster(8)
clusterExport(cl, lsf.str())
clusterEvalQ(cl, library(tidyverse))
clusterEvalQ(cl, library(tidytext))
clusterEvalQ(cl, library(tm))
clusterEvalQ(cl, library(openNLP))
clusterEvalQ(cl, library(data.table))
df["text"] <- parLapply(cl, df["text"], POSstring)
stopCluster(cl)
df
}
system.time(POSDFparallel(base_chunks[[1]]))
test <- POSDFparallel(base_chunks[[1]])
View(test)
test <- test %>% CleanTokens(n = 1) %>% group_by(document, word) %>% summarise(total = n())
View(test)
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
options(java.parameters = "-Xmx8000m")
registerDoParallel(cores = 8)
cl <- makeCluster(8)
clusterExport(cl, lsf.str())
clusterEvalQ(cl, library(tidyverse))
clusterEvalQ(cl, library(tidytext))
clusterEvalQ(cl, library(tm))
clusterEvalQ(cl, library(openNLP))
clusterEvalQ(cl, library(data.table))
POS_summary <- foreach(index = 1:length(base_chunks)) %dopar%
POSsumDFparallel(base_chunks[[index]], cl)
stopCluster(cl)
options(java.parameters = "-Xmx8000m")
registerDoParallel(cores = 8)
cl <- makeCluster(8)
clusterExport(cl, lsf.str())
clusterEvalQ(cl, library(tidyverse))
clusterEvalQ(cl, library(tidytext))
clusterEvalQ(cl, library(tm))
clusterEvalQ(cl, library(openNLP))
clusterEvalQ(cl, library(data.table))
clusterEvalQ(cl, library(doParallel))
POS_summary <- foreach(index = 1:length(base_chunks)) %dopar%
POSsumDFparallel(base_chunks[[index]], cl)
stopCluster(cl)
?parLapply
options(java.parameters = "-Xmx8000m")
registerDoParallel(cores = 8)
cl <- makeCluster(8)
clusterExport(cl, lsf.str())
clusterEvalQ(cl, library(tidyverse))
clusterEvalQ(cl, library(tidytext))
clusterEvalQ(cl, library(tm))
clusterEvalQ(cl, library(openNLP))
clusterEvalQ(cl, library(data.table))
clusterEvalQ(cl, library(doParallel))
POS_summary <- parLapply(cl, base_chunks, POSsumDFparallel)
stopCluster(cl)
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
options(java.parameters = "-Xmx8000m")
registerDoParallel(cores = 8)
cl <- makeCluster(8)
clusterExport(cl, lsf.str())
clusterEvalQ(cl, library(tidyverse))
clusterEvalQ(cl, library(tidytext))
clusterEvalQ(cl, library(tm))
clusterEvalQ(cl, library(openNLP))
clusterEvalQ(cl, library(data.table))
clusterEvalQ(cl, library(doParallel))
POS_summary <- parLapply(cl, base_chunks, POSsumDFparallel, cluster = cl)
stopCluster(cl)
options(java.parameters = "-Xmx8000m")
registerDoParallel(cores = 8)
cl <- makeCluster(8)
clusterExport(cl, lsf.str())
clusterEvalQ(cl, library(tidyverse))
clusterEvalQ(cl, library(tidytext))
clusterEvalQ(cl, library(tm))
clusterEvalQ(cl, library(openNLP))
clusterEvalQ(cl, library(data.table))
clusterEvalQ(cl, library(doParallel))
POS_summary <- parLapply(cl, base_chunks, POSsumDF)
stopCluster(cl)
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
# working directory
setwd("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing")
# modules
library(tidyverse)
library(tidytext)
library(tm)
library(openNLP)
library(data.table)
library(parallel)
library(doParallel)
# helper functions
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
# Load tidy dtm
load("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/full_tidy_dtm_ALL.RData")
setDT(full_tidy_dtm_1)
setDT(full_tidy_dtm_2)
setDT(full_tidy_dtm_3)
base_files <- LoadFiles("data/en_US/")
for (i in 1:8){}
base_chunks <- UnlistDF(base_files, size = 5e5)
for (i in 1:8){
options(java.parameters = "-Xmx8000m")
registerDoParallel(cores = i)
cl <- makeCluster(i)
clusterExport(cl, lsf.str())
clusterEvalQ(cl, library(tidyverse))
clusterEvalQ(cl, library(tidytext))
clusterEvalQ(cl, library(tm))
clusterEvalQ(cl, library(openNLP))
clusterEvalQ(cl, library(data.table))
clusterEvalQ(cl, library(doParallel))
system.time(parLapply(cl, base_chunks[[1]], POSsumDF))
stopCluster(cl)
}
base_chunks[[1]]
for (i in 1:8){
options(java.parameters = "-Xmx8000m")
registerDoParallel(cores = i)
cl <- makeCluster(i)
clusterExport(cl, lsf.str())
clusterEvalQ(cl, library(tidyverse,
tidytext,
tm,
openNLP,
data.table,
doParallel))
system.time(parLapply(cl, base_chunks[[1]], POSsumDF))
stopCluster(cl)
}
library(openNLP)
for (i in 1:8){
options(java.parameters = "-Xmx8000m")
registerDoParallel(cores = i)
cl <- makeCluster(i)
clusterExport(cl, lsf.str())
clusterEvalQ(cl, library(tidyverse,
tidytext,
tm,
openNLP,
data.table,
doParallel))
system.time(parLapply(cl, base_chunks[[1]], POSsumDF))
stopCluster(cl)
}
for (i in 1:8){
options(java.parameters = "-Xmx8000m")
registerDoParallel(cores = i)
cl <- makeCluster(i)
clusterExport(cl, lsf.str())
clusterEvalQ(cl, library(tidyverse,
tidytext,
tm,
data.table,
doParallel))
system.time(parLapply(cl, base_chunks[[1]], POSsumDF))
stopCluster(cl)
}
for (i in 1:8){
options(java.parameters = "-Xmx8000m")
registerDoParallel(cores = i)
cl <- makeCluster(i)
clusterExport(cl, lsf.str())
clusterEvalQ(cl, library(tidyverse,
tidytext,
tm))
clusterEvalQ(cl,library(openNLP,
data.table,
doParallel))
system.time(parLapply(cl, base_chunks[[1]], POSsumDF))
stopCluster(cl)
}
for (i in 1:8){
options(java.parameters = "-Xmx8000m")
registerDoParallel(cores = i)
cl <- makeCluster(i)
clusterExport(cl, lsf.str())
clusterEvalQ(cl, library(tidyverse))
clusterEvalQ(cl, library(tidytext))
clusterEvalQ(cl, library(tm))
clusterEvalQ(cl, library(openNLP))
clusterEvalQ(cl, library(data.table))
clusterEvalQ(cl, library(doParallel))
system.time(parLapply(cl, base_chunks[[1]], POSsumDF))
stopCluster(cl)
}
for (i in 1:8){
options(java.parameters = "-Xmx8000m")
registerDoParallel(cores = i)
cl <- makeCluster(i)
clusterExport(cl, lsf.str())
clusterEvalQ(cl, library(tidyverse))
clusterEvalQ(cl, library(tidytext))
clusterEvalQ(cl, library(tm))
clusterEvalQ(cl, library(openNLP))
clusterEvalQ(cl, library(data.table))
clusterEvalQ(cl, library(doParallel))
system.time(parLapply(cl, base_chunks[[1]]["text"], POSstring))
stopCluster(cl)
}
View(base_files)
system.time(1+1)
str(system.time(1+1))
print(system.time(1+1))
for (i in 1:8){
options(java.parameters = "-Xmx8000m")
registerDoParallel(cores = i)
cl <- makeCluster(i)
clusterExport(cl, lsf.str())
clusterEvalQ(cl, library(tidyverse))
clusterEvalQ(cl, library(tidytext))
clusterEvalQ(cl, library(tm))
clusterEvalQ(cl, library(openNLP))
clusterEvalQ(cl, library(data.table))
clusterEvalQ(cl, library(doParallel))
print(i)
print(system.time(parLapply(cl, base_chunks[[1]]["text"], POSstring)))
stopCluster(cl)
}
system.time(lapply(base_chunks[[1]]["text"], POSstring))
gc()
gc()
base_chunks <- UnlistDF(LoadFiles("data/en_US/"), size = 1e4)
options(java.parameters = "-Xmx8000m")
registerDoParallel(cores = 8)
cl <- makeCluster(8)
clusterExport(cl, lsf.str())
clusterEvalQ(cl, library(tidyverse))
clusterEvalQ(cl, library(tidytext))
clusterEvalQ(cl, library(tm))
clusterEvalQ(cl, library(openNLP))
clusterEvalQ(cl, library(data.table))
clusterEvalQ(cl, library(doParallel))
POS_summary <- parLapply(cl, base_chunks, POSsumDF)
stopCluster(cl)
options(java.parameters = "-Xmx16000m")
POS_summary_list <- lapply(base_chunks, POSsumDF)
chunk_number = 50
base_chunks_chunks <- split(base_chunks, cut(seq_along(base_chunks),chunk_number,labels = FALSE))
View(base_chunks_chunks)
gc()
# working directory
setwd("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing")
# modules
library(tidyverse)
library(tidytext)
library(tm)
library(openNLP)
library(data.table)
library(parallel)
library(doParallel)
# helper functions
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
# Load tidy dtm
load("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/full_tidy_dtm_ALL.RData")
setDT(full_tidy_dtm_1)
setDT(full_tidy_dtm_2)
setDT(full_tidy_dtm_3)
base_chunks <- UnlistDF(LoadFiles("data/en_US/"), size = 1e5)
chunk_number = 50
base_chunks_chunks <- split(base_chunks,
cut(seq_along(base_chunks),
chunk_number,
labels = FALSE))
rm(base_chunks)
length(base_chunks_chunks)
View(base_chunks_chunks)
base_chunks_chunks[1]
base_chunks_chunks[[1]]
View(base_chunks_chunks[[1]])
POS_summary_list <- list()
for (index in 1:length(base_chunks_chunks)){
gc()
options(java.parameters = "-Xmx16000m")
temp_list_ <- lapply(base_chunks_chunks[[index]], POSsumDF)
POS_summary_list <- c(POS_summary_list, MergeDTM(temp_list_))
rm(temp_list_)
}
POS_summary_list <- list()
for (index in 1:length(base_chunks_chunks)){
gc()
options(java.parameters = "-Xmx16000m")
temp_list_ <- lapply(base_chunks_chunks[[index]], POSsumDF)
POS_summary_list <- c(POS_summary_list, MergeDTM(temp_list_))
rm(temp_list_)
print(index)
}
