setwd("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing")
library(tidyverse)
library(tidytext)
library(tm)
library(ggthemes)
library(openNLP)
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
base_1000 <- LoadFiles("data/en_US/", n = 1000)
tidy_1000 <- lapply(base_1000, CleanTokens, n = 1)
tidy_1_1000 <- MergeDTM(tidy_1000)
View(tidy_1_1000)
unique_words <- tidy_1_1000 %>% group_by(word)
View(unique_words)
unique_words <- tidy_1_1000 %>% group_by(word) %>% summarise(freq = sum(n))
View(unique_words)
unique_words <- unique_words %>% mutate(p = freq/sum(freq))
tidy_2000 <- lapply(base_1000, CleanTokens, n = 2)
tidy_2_1000 <- MergeDTM(tidy_2000)
View(tidy_2_1000)
tidy_2_1000 <- tidy_2_1000 %>% separate(word,
c("word1", "word2"),
sep = " ")
unique_bigrams <- tidy_2_1000 %>% group_by(word1, word2) %>% summarise(freq = sum(n))
unique_words <- unique_bigrams %>% mutate(p = freq/sum(freq))
unique_bigrams <- unique_bigrams %>% mutate(p = freq/sum(freq))
unique_unigrams <- tidy_1_1000 %>% group_by(word) %>% summarise(freq = sum(n)) %>% mutate(p = freq / sum(freq))
View(unique_words)
View(unique_bigrams)
View(unique_bigrams)
View(unique_unigrams)
unique_bigrams %>% group_by(word1) %>% summarise(p = p)
PercentLexicon(unique_bigrams)
unique_bigrams <- ungroup(unique_bigrams)
PercentLexicon(unique_bigrams)
PercemtLexicon(tidy_2_1000)
PercentLexicon(tidy_2_1000)
View(tidy_2_1000)
PercentLexicon(tidy_2_1000[,-3])
PercentLexicon(select(tidy_2_1000, -c("word1")))
View(unique_unigrams)
predictor_table <- tibble(word1 = c(), word2 = c(), word3 = c())
View(predictor_table)
spread(unique_unigrams)
spread(unique_unigrams, word, p)
PercentLexicon(tidy_1000)
PercentLexicon(tidy_1_1000)
tidy_1_1000 %>% bind_tf_idf(word, document, n) %>% arrange(desc(tf)) %>% mutate(cumulative_sum = cumsum(tf)) %>% summarise('n 25%' = sum(cumulative_sum <= 0.25),
'n 50%' = sum(cumulative_sum <= 0.5),
'n 75%' = sum(cumulative_sum <= 0.75),
'n 90%' = sum(cumulative_sum <= 0.9),
'n 100%'= n())
spread(select_head(arrange(unique_unigrams, desc(p)), n = 1000), word, p)
spread(slice_head(arrange(unique_unigrams, desc(p)), n = 1000), word, p)
spread(slice_head(arrange(unique_unigrams, desc(p)), n = 1000), word, p, freq)
spread(slice_head(arrange(unique_unigrams, desc(p)), n = 1000), word, p, n)
spread(slice_head(arrange(unique_unigrams, desc(p)), n = 1000), word, p)
test <- spread(slice_head(arrange(unique_unigrams, desc(p)), n = 1000), word, p)
View(test)
test <- spread(slice_head(arrange(select(unique_unigrams, -freq), desc(p)), n = 1000), word, p)
View(test)
predictor_table[1] <- c("", "", "")
predictor_table[1] <- c(word1: "word1", word2:"word2", word3:"word3")
predictor_table <- tibble(word1 = c("word1"), word2 = c("word2"), word3 = c("word3"))
View(predictor_table)
predictor_table <- cbind(predictor_table, test)
unigrams <- unique_unigrams$word
unigrams
unigrams <- slice_head(arrange(select(unique_unigrams, -freq), desc(p)), n = 1000)$word
unigrams
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
UnigramProb(tidy_1_1000)
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
UnigramProb(tidy_1_1000)
UnigramProb(tidy_1_1000, n = 1000)
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
UnigramProb(tidy_1_1000, n = 1000)
UnigramProb(tidy_1_1000)
UnigramProb(tidy_2_1000, n = 1000)
tidy_2000 <- lapply(base_1000, CleanTokens, n = 2)
UnigramProb(tidy_2_1000, n = 1000)
UnigramProb(tidy_2000, n = 1000)
tidy_2_1000 <- MergeDTM(tidy_2000)
UnigramProb(tidy_2_1000, n = 1000)
UnigramProb(tidy_2_1000, n = 10000)
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
View(UnigramProb)
uni_freq <- WordFreqProb(tidy_1_1000, n = 1000)
View(uni_freq)
bi_freq <- WordFreqProb(tidy_2_1000, n = 10000)
View(bi_freq)
bi_freq <- bi_freq %>% separate(word, c("word1", "word2", sep = " "))
bi_freq <- bi_freq %>% separate(word, c("word1", "word2"), sep = " ")
bi_freq <- WordFreqProb(tidy_2_1000, n = 10000)
bi_freq <- bi_freq %>% separate(word, c("word1", "word2"), sep = " ")
bi_freq <- select(bi_freq, -p)
merge(bi_freq, uni_freq, by = c("word1", "word")
)
bi_freq %>% left_join(uni_freq, by = "word")
bi_freq %>% left_join(uni_freq, by = c("word1", "word"))
bi_freq %>% left_join(uni_freq, by = c("word", "word1"))
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
