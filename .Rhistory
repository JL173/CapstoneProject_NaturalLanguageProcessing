setwd("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing")
library(tidyverse)
library(tidytext)
library(tm)
library(ggthemes)
library(openNLP)
library(R.utils)
library(data.table)
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
load("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/tidy_4_stem.RDa")
quagram_ <- WordFreqProb(tidy_4_)
save(quagram_, file = "full_quagram.RDa")
gc()
load("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/tidy_3_stem.RDa")
trigram_ <- WordFreqProb(tidy_3_)
save(trigram_, file = "full_trigram.RDa")
gc()
load("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/tidy_2_stem.RDa")
bigram_ <- WordFreqProb(tidy_2_)
save(bigram_, file = "full_bigram.RDa")
gc()
load("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/tidy_1_stem.RDa")
unigram_ <- WordFreqProb(tidy_1_)
save(unigram_, file = "full_unigram.RDa")
gc()
vocab_n_ <- unigram_ %>%
mutate(cumsum = cumsum(P)) %>%
summarise(vocab = sum(cumsum <= 0.9))%>%
as.integer()
unigram_ <- unigram_ %>% slice_head(n = vocab_n_)
model <- CreateProbTable(unigram_,
bigram_,
trigram_,
quagram_)
setwd("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing")
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
library(tidyverse)
library(R.utils)
library(data.table)
load("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/full_unigram.RDa")
unigram_ <- unigram_ %>% filter(freq > 3)
gc()
load("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/full_bigram.RDa")
bigram_ <- bigram_ %>% filter(freq > 2)
gc()
load("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/full_trigram.RDa")
trigram_ <- trigram_ %>% filter(freq > 2)
gc()
load("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/full_quagram.RDa")
quagram_ <- quagram_ %>% filter(freq > 1)
gc()
model <- CreateProbTable(unigram_,
bigram_,
trigram_,
quagram_)
save(model, file = "model_stem2.RDa")
for (index in 1:10){
print(test_strings[index])
print("")
print(StringTailngram(test_strings[index], stem = TRUE, filter = stop_words))
print("")
print(MatchStringPredict(test_strings[index],
model = model,
n = 10,
stem = TRUE))
#filter = stop_words))
print("")
}
# coursera quiz predictions
test_strings <- c("The guy in front of me just bought a pound of bacon, a bouquet, and a case of", "You're the reason why I smile everyday. Can you follow me please? It would mean the", "Hey sunshine, can you follow me and make me the", "Very early observations on the Bills game: Offense still struggling but the","Go on a romantic date at the", "Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my", "Ohhhhh #PointBreak is on tomorrow. Love that film and haven't seen it in quite some", "After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little", "Be grateful for the good times and keep the faith during the", "If this isn't the cutest thing you've ever seen, then you must be")
for (index in 1:10){
print(test_strings[index])
print("")
print(StringTailngram(test_strings[index], stem = TRUE, filter = stop_words))
print("")
print(MatchStringPredict(test_strings[index],
model = model,
n = 10,
stem = TRUE))
#filter = stop_words))
print("")
}
library(tidyverse)
library(tidytext)
library(tm)
library(ggthemes)
library(openNLP)
library(R.utils)
library(data.table)
for (index in 1:10){
print(test_strings[index])
print("")
print(StringTailngram(test_strings[index], stem = TRUE, filter = stop_words))
print("")
print(MatchStringPredict(test_strings[index],
model = model,
n = 10,
stem = TRUE))
#filter = stop_words))
print("")
}
for (index in 1:10){
print(test_strings[index])
print("")
print(StringTailngram(test_strings[index], stem = TRUE, filter = stop_words))
print("")
print(MatchStringPredict(test_strings[index],
model = model,
table = TRUE,
n = 10,
stem = TRUE))
#filter = stop_words))
print("")
}
for (index in 1:10){
print(test_strings[index])
print("")
print(StringTailngram(test_strings[index], stem = TRUE))
print("")
print(MatchStringPredict(test_strings[index],
model = model,
table = TRUE,
n = 10,
stem = TRUE))
#filter = stop_words))
print("")
}
for (index in 1:10){
print(test_strings[index])
print("")
print(StringTailngram(test_strings[index], stem = TRUE))
print("")
print(MatchStringPredict(test_strings[index],
model = model,
table = TRUE,
n = 20,
stem = TRUE))
#filter = stop_words))
print("")
}
for (index in 1:10){
print(test_strings[index])
print("")
print(StringTailngram(test_strings[index], stem = TRUE))
print("")
print(MatchStringPredict(test_strings[index],
model = model,
table = TRUE,
n = 50,
stem = TRUE))
#filter = stop_words))
print("")
}
for (index in 1:10){
print(test_strings[index])
print("")
print(StringTailngram(test_strings[index], stem = TRUE))
print("")
print(MatchStringPredict(test_strings[index],
model = model,
table = TRUE,
n = 100,
stem = TRUE))
#filter = stop_words))
print("")
}
model %>% filter(word1 == "faith", word2 = "during", word3 = "the") %>% head(10)
model %>% filter(word1 == "faith", word2 == "during", word3 == "the") %>% head(10)
model %>% filter(word1 == "during", word2 == "the") %>% head(10)
model %>% filter(word1 == "callous") %>% head(10)
model %>% filter(word1 == "be", word2 == "callous") %>% head(10)
model %>% filter(word1 == "be", word2 == "insensitive") %>% head(10)
model %>% filter(word2 == "be", word3 == "insensitive") %>% head(10)
model %>% filter(word2 == "be", word3 == "insane") %>% head(10)
model %>% filter(word2 == "be", word3 == "asleep") %>% head(10)
model %>% filter(word2 == "must", word3 == "be") %>% head(10)
model %>% filter(word2 == "must", word3 == "be", word4 == "insane") %>% head(10)
model %>% filter(word2 == "must", word3 == "be") %>% head(10)
model %>% filter(word2 == "must", word3 == "be", word4 == "insane") %>% head(10)
model %>% filter(word2 == "must", word3 == "be") %>% head(10)
model %>% filter(word2 == "be", word3 == "insane") %>% head(10)
model %>% filter(word2 == "must", word3 == "be") %>% head(10)
model %>% filter(word2 == "must", word3 == "be", word4 != NA) %>% head(10)
model %>% filter(word2 == "during", word3 == "the") %>% head(10)
model %>% filter(word1 == "during", word2 == "the") %>% head(10)
model %>% filter(word1 == "faith", word2 == "during") %>% head(10)
model %>% filter(word1 == "during", word2 == "the") %>% head(10)
model %>% filter(word2 == "bad", word1 == "the") %>% head(10)
model %>% filter(word2 == "sad", word1 == "the") %>% head(10)
model %>% filter(word2 == "worse", word1 == "the") %>% head(10)
model %>% filter(word2 == "hard", word1 == "the") %>% head(10)
model %>% filter(word2 == "his", word1 == "with", word3 == "little") %>% head(10)
model %>% filter(word1 == "his", word2 == "little") %>% head(10)
model %>% filter(word2 == "his", word3 == "little") %>% head(10)
model %>% filter(word4 == "little") %>% head(10)
model %>% filter(word3 == "his", word4 == "little") %>% head(10)
model %>% filter(word2 == "his", word3 == "little") %>% head(10)
model %>% filter(word1 == "his", word2 == "little") %>% head(10)
model %>% filter(word1 == "little") %>% head(10)
model %>% filter(word1 == "little", word2 == "toes") %>% head(3)
model %>% filter(word1 == "little", word2 == "fingers") %>% head(3)
model %>% filter(word1 == "little", word2 == "fing") %>% head(3)
model %>% filter(word1 == "little", word2 == "fin") %>% head(3)
model %>% filter(word1 == "little", word2 == "eyes") %>% head(3)
model %>% filter(word1 == "little", word2 == "ears") %>% head(3)
model %>% filter(word1 == "little", word2 == "ear") %>% head(3)
model %>% filter(word1 == "little", word2 == "eye") %>% head(3)
model %>% filter(word1 == "little", word2 == "ey") %>% head(3)
model %>% filter(word1 == "little", word2 == "e") %>% head(3)
model %>% filter(word1 == "little", word2 == "tp") %>% head(3)
model %>% filter(word1 == "little", word2 == "to") %>% head(3)
model %>% filter(word1 == "struggling") %>% head(3)
model %>% filter(word1 == "but", word2 == "the") %>% head(3)
model %>% filter(word1 == "but", word2 == "the") %>% head(10)
model %>% filter(word1 == "struggling", word2 == "but") %>% head(10)
model %>% filter(word2 == "but", word3 == "the") %>% head(10)
model %>% filter(word2 == "but", word3 == "the", word4 != NA) %>% head(10)
model %>% filter(word2 == "but", word3 == "the") %>% drop_na() %>% head(10)
model %>% filter(word2 == "but", word3 == "the") %>% drop_na() %>% head(20)
model %>% filter(word2 == "but", word3 == "the", word4 == "players") %>% drop_na() %>% head(20)
model %>% filter(word2 == "but", word3 == "the", word4 == "crowd") %>% drop_na() %>% head(20)
model %>% filter(word2 == "but", word3 == "the", word4 == "defense") %>% drop_na() %>% head(20)
model %>% filter(word2 == "but", word3 == "the", word4 == "referees") %>% drop_na() %>% head(20)
model %>% filter(word1 == "but", word2 == "the", word3 == "referees") %>% drop_na() %>% head(10)
model %>% filter(word1 == "but", word2 == "the", word3 == "defense") %>% drop_na() %>% head(10)
model %>% filter(word1 == "but", word2 == "the", word3 == "crowd") %>% drop_na() %>% head(10)
model %>% filter(word1 == "struggl") %>% drop_na() %>% head(10)
model %>% filter(word1 == "struggl") %>% head(10)
model %>% filter(word1 == "struggl", word2 == "but") %>% drop_na()%>% head(10)
model %>% filter(word1 == "but", word2 == "the") %>% drop_na()%>% head(10)
model %>% filter(word1 == "but", word2 == "the", word3 == "players") %>% drop_na()%>% head(10)
model %>% filter(word1 == "but", word2 == "the", word3 == "crowd") %>% drop_na()%>% head(10)
model %>% filter(word1 == "but", word2 == "the", word3 == "defense") %>% drop_na()%>% head(10)
model %>% filter(word1 == "the", word2 == "defense") %>% drop_na()%>% head(10)
model %>% filter(word1 == "the", word2 == "referees") %>% drop_na()%>% head(10)
model %>% filter(word1 == "the", word2 == "referee") %>% drop_na()%>% head(10)
model %>% filter(word1 == "the", word2 == "refere") %>% drop_na()%>% head(10)
model %>% filter(word1 == "the", word2 == "defens") %>% drop_na()%>% head(10)
model %>% filter(word1 == "fingers") %>% drop_na()%>% head(10)
model %>% filter(word1 == "finger") %>% drop_na()%>% head(10)
model %>% filter(word1 == "finge") %>% drop_na()%>% head(10)
model %>% filter(word1 == "fing") %>% drop_na()%>% head(10)
model %>% filter(word1 == "finger") %>% drop_na()%>% head(10)
model %>% filter(word12 == "finger") %>% drop_na()%>% head(10)
model %>% filter(word2 == "finger") %>% drop_na()%>% head(10)
model %>% filter(word1 == "littl", word2 == "finger") %>% drop_na()%>% head(10)
model %>% filter(word1 == "little", word2 == "finger") %>% drop_na()%>% head(10)
model %>% filter(word1 == "littl", word2 == "fingers") %>% drop_na()%>% head(10)
model %>% filter(word2 == "littl", word3 == "fingers") %>% drop_na()%>% head(10)
model %>% filter(word3 == "fingers") %>% drop_na()%>% head(10)
model %>% filter(word3 == "finger") %>% drop_na()%>% head(10)
model %>% filter(word2 == "little", word3 == "finger") %>% drop_na()%>% head(10)
model %>% filter(word2 == "littl", word3 == "finger") %>% drop_na()%>% head(10)
for (index in 1:10){
print(test_strings[index])
print("")
print(StringTailngram(test_strings[index], stem = TRUE))
print("")
print(MatchStringPredict(test_strings[index],
model = model,
table = TRUE,
n = -1L,
stem = TRUE))
#filter = stop_words))
print("")
}
for (index in 1:10){
print(test_strings[index])
print("")
print(StringTailngram(test_strings[index], stem = TRUE))
print("")
print(MatchStringPredict(test_strings[index],
model = model,
table = TRUE,
n = 1000,
stem = TRUE))
#filter = stop_words))
print("")
}
for (index in 1:10){
print(test_strings[index])
print("")
print(StringTailngram(test_strings[index], stem = TRUE))
print("")
print(MatchStringPredict(test_strings[index],
model = model,
table = TRUE,
n = 100
,
stem = TRUE))
#filter = stop_words))
print("")
}
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
for (index in 1:10){
print(test_strings[index])
print("")
print(StringTailngram(test_strings[index], stem = TRUE))
print("")
print(MatchStringPredict(test_strings[index],
model = model,
table = TRUE,
n = 10
,
stem = TRUE))
#filter = stop_words))
print("")
}
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
for (index in 1:10){
print(test_strings[index])
print("")
print(StringTailngram(test_strings[index], stem = TRUE))
print("")
print(MatchStringPredict(test_strings[index],
model = model,
table = TRUE,
n = 10
,
stem = TRUE))
#filter = stop_words))
print("")
}
for (index in 1:10){
print(test_strings[index])
print("")
print(StringTailngram(test_strings[index], stem = TRUE))
print("")
print(MatchStringPredict(test_strings[index],
model = model,
table = TRUE,
n = 1000
,
stem = TRUE))
#filter = stop_words))
print("")
}
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
for (index in 1:10){
print(test_strings[index])
print("")
print(StringTailngram(test_strings[index], stem = TRUE))
print("")
print(MatchStringPredict(test_strings[index],
model = model,
table = TRUE,
n = 1000
,
stem = TRUE))
#filter = stop_words))
print("")
}
source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper_functions.R")
for (index in 1:10){
print(test_strings[index])
print("")
print(StringTailngram(test_strings[index], stem = TRUE))
print("")
print(MatchStringPredict(test_strings[index],
model = model,
table = TRUE,
n = 1000
,
stem = TRUE))
#filter = stop_words))
print("")
}
for (index in 1:10){
print(test_strings[index])
print("")
print(StringTailngram(test_strings[index], stem = TRUE))
print("")
print(MatchStringPredict(test_strings[index],
model = model,
table = TRUE,
n = 10
,
stem = TRUE))
#filter = stop_words))
print("")
}
