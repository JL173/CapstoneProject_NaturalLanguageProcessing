---
title: "R Notebook 2"
output: html_notebook
---

```{r}
# coursera quiz predictions

test_strings2 <- c("When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd", "Guy at my table's wife got up to go to the bathroom and I asked about dessert and he started telling me about his", "I'd give anything to see arctic monkeys this", "Talking to your mom has the same effect as a hug and helps reduce your", "When you were in Holland you were like 1 inch away from me but you hadn't time to take a", "I'd just like all of these questions answered, a presentation of evidence, and a jury to settle the", "I can't deal with unsymetrical things. I can't even hold an uneven number of bags of groceries in each", "Every inch of you is perfect from the bottom to the", "I’m thankful my childhood was filled with imagination and bruises from playing", "I like how the same people are in almost all of Adam Sandler's")

```

```{r}
setwd("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing")

library(tidyverse)
library(tidytext)
library(tm)
library(ggthemes)
library(openNLP)
library(R.utils)
library(data.table)

source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper2.R")

data("stop_words")

```

# Alternate methods to explore

## Currently using Katz-smoothing

using alpha multiplying the rest (n-1)-grams for the rest of the probability distribution

## Good-Turing smoothing

n-gram occurs r times, it now occurs
(r + 1) * n + 1 / n   times

example, a 3-gram of freq = 20
now → 21 * 4/3 = 28 times

## Knesser-Nay smoothing

https://smithamilli.com/blog/kneser-ney/

## New Package for speed

http://davevinson.com/cmscu-tutorial.html

## Other datasets

*IMDB reviews, +ve and -ve*
https://ai.stanford.edu/~amaas/data/sentiment/

- has <br><br /> tags throughout that need removed

*Amazon product reviews*
https://www.cs.jhu.edu/~mdredze/datasets/sentiment/






```{r}
base <- LoadSingleFile("data/en_US/25/en_US.twitter.25.txt", SOS = TRUE, EOS = TRUE)

twitter_unigrams <- CleanTokens(base, n = 1) %>%
  WordFreq() %>%
  filter(freq > 2) 

twitter_bigrams <- CleanTokens(base, n = 2) %>%
  WordFreq() %>%
  filter(freq > 1)

twitter_trigrams <- CleanTokens(base, n = 3) %>%
  WordFreq() %>%
  filter(freq > 1)

twitter_quagrams <- CleanTokens(base, n = 4) %>%
  WordFreq() %>%
  filter(freq > 1)

twitter_model <- CreateFreqTable(twitter_unigrams,
                                 twitter_bigrams,
                                 twitter_trigrams,
                                 twitter_quagrams)

save(twitter_model, file = "models/twitter_freq_model.RDa")
```

```{r}
base_news <- LoadSingleFile("data/en_US/25/en_US.news.25.txt", SOS = TRUE, EOS = TRUE)

news_unigrams <- CleanTokens(base_news, n = 1) %>%
  WordFreq() %>%
  filter(freq > 2) 

news_bigrams <- CleanTokens(base_news, n = 2) %>%
  WordFreq() %>%
  filter(freq > 1)

news_trigrams <- CleanTokens(base_news, n = 3) %>%
  WordFreq() %>%
  filter(freq > 1)

news_quagrams <- CleanTokens(base_news, n = 4) %>%
  WordFreq() %>%
  filter(freq > 1)

news_model <- CreateFreqTable(news_unigrams,
                                 news_bigrams,
                                 news_trigrams,
                                 news_quagrams)

save(news_model, file = "models/news_freq_model.RDa")
```

```{r}
base_blogs <- LoadSingleFile("data/en_US/25/en_US.blogs.25.txt", SOS = TRUE, EOS = TRUE)

blogs_unigrams <- CleanTokens(base_blogs, n = 1) %>%
  WordFreq() %>%
  filter(freq > 2) 

blogs_bigrams <- CleanTokens(base_blogs, n = 2) %>%
  WordFreq() %>%
  filter(freq > 1)

blogs_trigrams <- CleanTokens(base_blogs, n = 3) %>%
  WordFreq() %>%
  filter(freq > 1)

blogs_quagrams <- CleanTokens(base_blogs, n = 4) %>%
  WordFreq() %>%
  filter(freq > 1)

blogs_model <- CreateFreqTable(blogs_unigrams,
                                 blogs_bigrams,
                                 blogs_trigrams,
                                 blogs_quagrams)

save(blogs_model, file = "models/blogs_freq_model.RDa")
```

```{r}
Predict("sos in the", twitter_model, ngram = 3, gamma = 0.5)
```

```{r}
for (string in test_strings2){
  results <- Predict(string, twitter_model, ngram = 3, gamma = 0.5) %>% head(10)
  print(results)
}
```

```{r}
base_files <- LoadFiles("data/en_US/10/")
```
```{r}
full_unigrams <- lapply(base_files, CleanTokens, n = 1) %>%
  MergeDTM() %>%
  WordFreq() %>%
  filter(freq > 1) 

full_bigrams <- lapply(base_files, CleanTokens, n = 2) %>%
  MergeDTM() %>%
  WordFreq() %>%
  filter(freq > 1)

full_trigrams <- lapply(base_files, CleanTokens, n = 3) %>%
  MergeDTM() %>%
  WordFreq() %>%
  filter(freq > 1)

full_quagrams <- lapply(base_files, CleanTokens, n = 4) %>%
  MergeDTM() %>%
  WordFreq() %>%
  filter(freq > 1)
```
```{r}
full_model_10 <- CreateFreqTable(full_unigrams, full_bigrams,
                                 full_trigrams, full_quagrams)
```
```{r}
for (string in test_strings2){
  results <- Predict(string, full_model_10, ngram = 3, gamma = 0.3) %>% head(10)
  print(results)
}
```

```{r}
for (string in test_strings2){
  results <- Predict(string, full_model, ngram = 3, gamma = 0.3) %>% head(100)
  print(results)
}
```


```{r}
t <- Sys.time()

imdb <- LoadFiles("data/IMDB")

n <- Sys.time() - t
print(n)
```

```{r}
t <- Sys.time()

imdb_unigrams <- lapply(imdb, CleanTokens, n = 1) %>%
  MergeDTM() %>%
  WordFreq() %>%
  filter(freq > 1)

n <- Sys.time() - t
print(n)
```

```{r}
t <- Sys.time()

imdb_bigrams <- lapply(imdb, CleanTokens, n = 2) %>%
  MergeDTM() %>%
  WordFreq() %>%
  filter(freq > 1)

n <- Sys.time() - t
print(n)
```


```{r}
t <- Sys.time()

imdb_trigrams <- lapply(imdb, CleanTokens, n = 3) %>%
  MergeDTM() %>%
  WordFreq() %>%
  filter(freq > 1)

n <- Sys.time() - t
print(n)
```

```{r}
t <- Sys.time()

imdb_quagrams <- lapply(imdb, CleanTokens, n = 4) %>%
  MergeDTM() %>%
  WordFreq() %>%
  filter(freq > 1)

n <- Sys.time() - t
print(n)
```


```{r}
t <- Sys.time()

imdb_model <- CreateFreqTable(imdb_unigrams,
                              imdb_bigrams,
                              imdb_trigrams,
                              imdb_quagrams)

n <- Sys.time() - t
print(n)
```


```{r}
for (string in test_strings2){
  results <- Predict(string, imdb_model, ngram = 3, gamma = 0.3) %>% head(20)
  print(results)
}
```

```{r}
load("output/full_unigram.RDa")
unigram_ <- unigram_ %>% filter(freq > 1)
load("output/full_bigram.Rda")
bigram_ <- bigram_ %>% filter(freq > 1)
load("output/full_trigram.Rda")
trigram_ <- trigram_ %>% filter(freq > 1)
load("output/full_quagram.RDa")
quagram_ <- quagram_ %>% filter(freq > 1)

en_US_model <- CreateFreqTable(unigram_,
                              bigram_,
                              trigram_,
                              quagram_)

save(en_US_model, file = "models/en_US_freq_model.RDa")
```

```{r}
# sticking two models together

load("models/imdb_freq_model.RDa")
load("models/en_US_freq_model.RDa")
```

```{r}
merged_model <- rbind(en_US_model, imdb_model, lyrics_model) %>%
  group_by(word1, word2, word3, word4) %>%
  summarise(freq = sum(freq)) %>%
  ungroup()
```


```{r}
for (string in test_strings2){
  results <- Predict(string, twittwe_model, ngram = 3, gamma = 0.3) %>% head(20)
  print(results)
}
```


```{r}
lyrics <- LoadSingleFile("data/ARTISTS/")
```

```{r}

lyrics_unigrams <- lapply(lyrics, CleanTokens, n = 1, filter_df = filter) %>%
  MergeDTM() %>%
  WordFreq() %>%
  filter(freq > 1)

lyrics_bigrams <- lapply(lyrics, CleanTokens, n = 2, filter_df = filter) %>%
  MergeDTM() %>%
  WordFreq() %>%
  filter(freq > 1)

lyrics_trigrams <- lapply(lyrics, CleanTokens, n = 3, filter_df = filter) %>%
  MergeDTM() %>%
  WordFreq() %>%
  filter(freq > 1)

lyrics_quagrams <- lapply(lyrics, CleanTokens, n = 4, filter_df = filter) %>%
  MergeDTM() %>%
  WordFreq() %>%
  filter(freq > 1)
```

```{r}
lyrics_model <- CreateFreqTable(lyrics_unigrams,
                                 lyrics_bigrams,
                                 lyrics_trigrams,
                                 lyrics_quagrams)
save(lyrics_model, file = "models/lyrics_freq_model.RDa")
```



```{r}
results <- Predict("from the bottom", news_model, ngram = 2, gamma = 0.3) %>% arrange(desc(P)) %>% head(5)
results
```

```{r}
results$word <- factor(results$word, levels = results$word[order(results$P, decreasing = FALSE)])


ggplot(results, aes(y = word, x = P)) + 
  theme_wsj() +
  scale_fill_tableau() +
  scale_colour_tableau() +
  geom_col(orientation = "y") +
  labs(title = "Predictions", x = "Probability", y = "Next Word")
```
