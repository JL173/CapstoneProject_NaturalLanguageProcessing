---
title: "R Notebook 2"
output: html_notebook
---

```{r}
# coursera quiz predictions

test_strings2 <- c("When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd", "Guy at my table's wife got up to go to the bathroom and I asked about dessert and he started telling me about his", "I'd give anything to see arctic monkeys this", "Talking to your mom has the same effect as a hug and helps reduce your", "When you were in Holland you were like 1 inch away from me but you hadn't time to take a", "I'd just like all of these questions answered, a presentation of evidence, and a jury to settle the", "I can't deal with unsymetrical things. I can't even hold an uneven number of bags of groceries in each", "Every inch of you is perfect from the bottom to the", "I’m thankful my childhood was filled with imagination and bruises from playing", "I like how the same people are in almost all of Adam Sandler's")

```

```{r}
setwd("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing")

library(tidyverse)
library(tidytext)
library(tm)
library(ggthemes)
library(openNLP)
library(R.utils)
library(data.table)

source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper2.R")

data("stop_words")

```

# Alternate methods to explore

## Currently using Katz-smoothing

using alpha multiplying the rest (n-1)-grams for the rest of the probability distribution

## Good-Turing smoothing

n-gram occurs r times, it now occurs
(r + 1) * n + 1 / n   times

example, a 3-gram of freq = 20
now → 21 * 4/3 = 28 times

## Knesser-Nay smoothing

https://smithamilli.com/blog/kneser-ney/

## New Package for speed

http://davevinson.com/cmscu-tutorial.html

## Other datasets

*IMDB reviews, +ve and -ve*
https://ai.stanford.edu/~amaas/data/sentiment/

- has <br><br /> tags throughout that need removed

*Amazon product reviews*
https://www.cs.jhu.edu/~mdredze/datasets/sentiment/






```{r}
base <- LoadSingleFile("data/en_US/25/en_US.twitter.25.txt", SOS = TRUE, EOS = TRUE)

twitter_unigrams <- CleanTokens(base, n = 1) %>%
  WordFreq() %>%
  filter(freq > 2) 

twitter_bigrams <- CleanTokens(base, n = 2) %>%
  WordFreq() %>%
  filter(freq > 1)

twitter_trigrams <- CleanTokens(base, n = 3) %>%
  WordFreq() %>%
  filter(freq > 1)

twitter_quagrams <- CleanTokens(base, n = 4) %>%
  WordFreq() %>%
  filter(freq > 1)

twitter_model <- CreateFreqTable(twitter_unigrams,
                                 twitter_bigrams,
                                 twitter_trigrams,
                                 twitter_quagrams)

save(twitter_model, file = "models/twitter_freq_model.RDa")
```

```{r}
base_news <- LoadSingleFile("data/en_US/25/en_US.news.25.txt", SOS = TRUE, EOS = TRUE)

news_unigrams <- CleanTokens(base_news, n = 1) %>%
  WordFreq() %>%
  filter(freq > 2) 

news_bigrams <- CleanTokens(base_news, n = 2) %>%
  WordFreq() %>%
  filter(freq > 1)

news_trigrams <- CleanTokens(base_news, n = 3) %>%
  WordFreq() %>%
  filter(freq > 1)

news_quagrams <- CleanTokens(base_news, n = 4) %>%
  WordFreq() %>%
  filter(freq > 1)

news_model <- CreateFreqTable(news_unigrams,
                                 news_bigrams,
                                 news_trigrams,
                                 news_quagrams)

save(news_model, file = "models/news_freq_model.RDa")
```

```{r}
base_blogs <- LoadSingleFile("data/en_US/25/en_US.blogs.25.txt", SOS = TRUE, EOS = TRUE)

blogs_unigrams <- CleanTokens(base_blogs, n = 1) %>%
  WordFreq() %>%
  filter(freq > 2) 

blogs_bigrams <- CleanTokens(base_blogs, n = 2) %>%
  WordFreq() %>%
  filter(freq > 1)

blogs_trigrams <- CleanTokens(base_blogs, n = 3) %>%
  WordFreq() %>%
  filter(freq > 1)

blogs_quagrams <- CleanTokens(base_blogs, n = 4) %>%
  WordFreq() %>%
  filter(freq > 1)

blogs_model <- CreateFreqTable(blogs_unigrams,
                                 blogs_bigrams,
                                 blogs_trigrams,
                                 blogs_quagrams)

save(blogs_model, file = "models/blogs_freq_model.RDa")
```

```{r}
base_files <- LoadFiles("data/en_US/10/")
```
```{r}
full_unigrams <- lapply(base_files, CleanTokens, n = 1) %>%
  MergeDTM() %>%
  WordFreq() %>%
  filter(freq > 1) 

full_bigrams <- lapply(base_files, CleanTokens, n = 2) %>%
  MergeDTM() %>%
  WordFreq() %>%
  filter(freq > 1)

full_trigrams <- lapply(base_files, CleanTokens, n = 3) %>%
  MergeDTM() %>%
  WordFreq() %>%
  filter(freq > 1)

full_quagrams <- lapply(base_files, CleanTokens, n = 4) %>%
  MergeDTM() %>%
  WordFreq() %>%
  filter(freq > 1)
```
```{r}
full_model_10 <- CreateFreqTable(full_unigrams, full_bigrams,
                                 full_trigrams, full_quagrams)
```



```{r}
load("output/full_unigram.RDa")
unigram_ <- unigram_ %>% filter(freq > 1)
load("output/full_bigram.Rda")
bigram_ <- bigram_ %>% filter(freq > 1)
load("output/full_trigram.Rda")
trigram_ <- trigram_ %>% filter(freq > 1)
load("output/full_quagram.RDa")
quagram_ <- quagram_ %>% filter(freq > 1)

en_US_model <- CreateFreqTable(unigram_,
                              bigram_,
                              trigram_,
                              quagram_)

save(en_US_model, file = "models/en_US_freq_model.RDa")
```


```{r}
secondfilter <- function(df){
  df <- df %>%
    filter(!str_detect(word, "nigg")) %>%
    filter(!str_detect(word, "fuck"))%>%
    filter(!str_detect(word, "bitc"))
  df
}
```

```{r}
lyrics_model <- CreateFreqTable(secondfilter(lyrics_unigrams),
                                 secondfilter(lyrics_bigrams),
                                 secondfilter(lyrics_trigrams),
                                 secondfilter(lyrics_quagrams))
save(lyrics_model, file = "models/lyrics_freq_model.RDa")
```

# From here I need to create two things

- Train/Test sets of the n-grams (trigrams, quagrams)
- Evaluating function for testing

We'll work with the twitter data to build up the function, then transfer it into the helper2.R file when complete

```{r}
base <- LoadSingleFile("data/en_US/25/en_US.twitter.25.txt", SOS = TRUE, EOS = TRUE)
```

```{r}
twitter_unigrams <- CleanTokens(base, n = 1) %>%
  WordFreq() %>%
  filter(freq > 2) 

twitter_bigrams <- CleanTokens(base, n = 2) %>%
  WordFreq() %>%
  filter(freq > 1)

twitter_trigrams <- CleanTokens(base, n = 3) %>%
  WordFreq() %>%
  filter(freq > 1)

twitter_quagrams <- CleanTokens(base, n = 4) %>%
  WordFreq() %>%
  filter(freq > 1)
```

## Here we create the test/train splits for trigrams and quagrams

- trigrams to check if the model can predict an unseen trigram
- quagrams to check if the model can predict an unseen quagram

We'll have to sample from each set of ngrams. This will force the model to have never seen them as it removes them from the training set of ngrams without replacement.

```{r}
TrainTestSplit <- function(df, percent = 0.75, seed = 17373){
  smp_size <- floor(percent * nrow(df))
  set.seed(seed)
  train_ind <- sample(seq_len(nrow(df)), size = smp_size)
  
  train <- df[train_ind, ]
  test <- df[-train_ind, ]
  
  results <- list(train = train, test = test)
}
```

```{r}
trigrams_ <-TrainTestSplit(twitter_trigrams, 0.75)

test_twitter_trigrams <- trigrams_$test
train_twitter_trigrams <- trigrams_$train
rm(trigrams_)

quagrams_ <-TrainTestSplit(twitter_quagrams, 0.75)

test_twitter_quagrams <- quagrams_$test
train_twitter_quagrams <- quagrams_$train
rm(quagrams_)
```

## Some of the trigrams have identical bigrams preceding

We'll need to filter these out so we have only one possible solution for each trigram
Likewise for identical trigrams preceding quagrams

```{r}
RemoveTestDuplicates <- function(df, ngram){
  
  if (ngram == 3){
    
    df <- df %>%
    separate(word, c("word1", "word2", "word3"), sep = " ") %>%
    select(c("word1", "word2", "word3", "freq"))
    df[, "word4"] <- NA
    
    df <- distinct(df, word1, word2, .keep_all = TRUE)
  } else if (ngram == 4){
    
    df <- df %>%
    separate(word, c("word1", "word2", "word3", "word4"), sep = " ") %>%
    select(c("word1", "word2", "word3", "word4", "freq"))
    
    df <- distinct(df, word1, word2, word3, .keep_all = TRUE)
  }
  
  df
}
```

```{r}
test_twitter_trigrams <- test_twitter_trigrams %>%
  RemoveTestDuplicates(ngram = 3) %>%
  filter(!is.na(word1))

test_twitter_quagrams <- test_twitter_quagrams %>%
  RemoveTestDuplicates(ngram = 4) %>% 
  filter(!is.na(word1))
```

## Now we have unique values for our testing sets

Now we must have an evaluating function that uses the 'trained model' to predict the next word (word3 for trigram, word4 for quagram) and check it against the actual word3/word4

- Grab the string to use for the predict function
We'll simply append the string to each row of the testset

- Grab both the word and the probability of the top result from the predict result
Create a function that calls Predict with parameters, but returns top result values
- Collect these results into another data.table
append these to the testset using lapply function of above

```{r}
CreatePredictionString <- function(df, ngram){
  
  if (ngram == 3){
    
    df <- df %>%
    unite(col = string, word1, word2, sep = " ",
          na.rm = TRUE, remove = FALSE)
    
  } else if (ngram == 4){
    
    df <- df %>%
    unite(col = string, word1, word2, word3, sep = " ",
          na.rm = TRUE, remove = FALSE)
  }
  
}
```

```{r}
test_twitter_trigrams <- test_twitter_trigrams %>%
  CreatePredictionString(ngram = 3)
test_twitter_quagrams <- test_twitter_quagrams %>%
  CreatePredictionString(ngram = 4)
```

```{r}
t <- Sys.time()
test_copy <- test_twitter_trigrams

test_strings <- test_copy[["string"]]

result_words <- c()
result_P <- c()
index = 0

for (string in test_strings){
  index =  index + 1
  if ((index %% 10) == 0){
    print(index / length(test_strings))
  }
  
  result <- Predict(string,
                    model = twitter_train_model,
                    ngram = 2,
                    gamma = 0.3) %>%
    head(1)

  result_words <- c(result_words, result[1, "word"])
  result_P <- c(result_P, result[1, "P"])
}

test_twitter_trigrams$predictions <- result_words
test_twitter_trigrams$probability <- result_P

n <- Sys.time()
t <- n - t
print(t)

save(test_twitter_trigrams, file = "output/test_twitter_trigrams.RDa")
```
```{r}
t <- Sys.time()
test_copy <- test_twitter_quagrams

test_strings <- test_copy[["string"]]

result_words <- c()
result_P <- c()
index = 0

for (string in test_strings){
  index =  index + 1
  print(index / length(test_strings))
  
  result <- Predict(string,
                    model = twitter_train_model,
                    ngram = 3,
                    gamma = 0.3) %>%
    head(1)
  
  result_words <- c(result_words, result[1, "word"])
  result_P <- c(result_P, result[1, "P"])
}

test_twitter_quagrams$predictions <- result_words
test_twitter_quagrams$probability <- result_P

n <- Sys.time()
t <- n - t
print(t)

save(test_twitter_quagrams, file = "output/test_twitter_quagrams.RDa")
```


```{r}
twitter_train_model <- CreateFreqTable(twitter_unigrams,
                                 twitter_bigrams,
                                 train_twitter_trigrams,
                                 train_twitter_quagrams)

#save(twitter_train_model, file = "models/twitter_train_model.RDa")
```

## Now a function that evaluates the accuracy, somehow

- We want to maximise Probability when correct
- Minimise probability when incorrect