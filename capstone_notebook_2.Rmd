---
title: "R Notebook 2"
output: html_notebook
---

```{r}
# coursera quiz predictions

test_strings2 <- c("When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd", "Guy at my table's wife got up to go to the bathroom and I asked about dessert and he started telling me about his", "I'd give anything to see arctic monkeys this", "Talking to your mom has the same effect as a hug and helps reduce your", "When you were in Holland you were like 1 inch away from me but you hadn't time to take a", "I'd just like all of these questions answered, a presentation of evidence, and a jury to settle the", "I can't deal with unsymetrical things. I can't even hold an uneven number of bags of groceries in each", "Every inch of you is perfect from the bottom to the", "Iâ€™m thankful my childhood was filled with imagination and bruises from playing", "I like how the same people are in almost all of Adam Sandler's")

```

```{r}
setwd("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing")

library(tidyverse)
library(tidytext)
library(tm)
library(ggthemes)
library(openNLP)
library(R.utils)
library(data.table)

source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper2.R")

data("stop_words")

```

```{r}
base <- LoadSingleFile("data/en_US/10/en_US.twitter.10.txt", SOS = TRUE, EOS = TRUE)
```

```{r}
twitter_unigrams <- CleanTokens(base, n = 1) %>%
  WordFreq() %>%
  filter(freq > 1) 

twitter_bigrams <- CleanTokens(base, n = 2) %>%
  WordFreq() %>%
  filter(freq > 1)

twitter_trigrams <- CleanTokens(base, n = 3) %>%
  WordFreq() %>%
  filter(freq > 1)

twitter_quagrams <- CleanTokens(base, n = 4) %>%
  WordFreq() %>%
  filter(freq > 1)
```

```{r}
twitter_model <- CreateFreqTable(twitter_unigrams,
                                 twitter_bigrams,
                                 twitter_trigrams,
                                 twitter_quagrams)
```

```{r}
Predict("sos in the", twitter_model, ngram = 3, gamma = 0.5)
```

```{r}
for (string in test_strings2){
  results <- Predict(string, twitter_model, ngram = 3, gamma = 0.5) %>% head(10)
  print(results)
}
```

```{r}
base_files <- LoadFiles("data/en_US/10/")
```
```{r}
full_unigrams <- lapply(base_files, CleanTokens, n = 1) %>%
  MergeDTM() %>%
  WordFreq() %>%
  filter(freq > 1) 

full_bigrams <- lapply(base_files, CleanTokens, n = 2) %>%
  MergeDTM() %>%
  WordFreq() %>%
  filter(freq > 1)

full_trigrams <- lapply(base_files, CleanTokens, n = 3) %>%
  MergeDTM() %>%
  WordFreq() %>%
  filter(freq > 1)

full_quagrams <- lapply(base_files, CleanTokens, n = 4) %>%
  MergeDTM() %>%
  WordFreq() %>%
  filter(freq > 1)
```
```{r}
full_model_10 <- CreateFreqTable(full_unigrams, full_bigrams,
                                 full_trigrams, full_quagrams)
```
```{r}
for (string in test_strings2){
  results <- Predict(string, full_model_10, ngram = 3, gamma = 0.3) %>% head(10)
  print(results)
}
```
