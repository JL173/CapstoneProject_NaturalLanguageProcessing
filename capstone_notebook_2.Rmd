---
title: "R Notebook 2"
output: html_notebook
---

```{r}
# coursera quiz predictions

test_strings2 <- c("When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd", "Guy at my table's wife got up to go to the bathroom and I asked about dessert and he started telling me about his", "I'd give anything to see arctic monkeys this", "Talking to your mom has the same effect as a hug and helps reduce your", "When you were in Holland you were like 1 inch away from me but you hadn't time to take a", "I'd just like all of these questions answered, a presentation of evidence, and a jury to settle the", "I can't deal with unsymetrical things. I can't even hold an uneven number of bags of groceries in each", "Every inch of you is perfect from the bottom to the", "Iâ€™m thankful my childhood was filled with imagination and bruises from playing", "I like how the same people are in almost all of Adam Sandler's")

```

```{r}
setwd("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing")

library(tidyverse)
library(tidytext)
library(tm)
library(ggthemes)
library(openNLP)
library(R.utils)
library(data.table)

source("C:/Users/JL/Desktop/Study/Coursera/Johns Hopkins Data Science/10 capstone/CapstoneProject_NaturalLanguageProcessing/helper2.R")

data("stop_words")

```

```{r}
base <- LoadSingleFile("data/en_US/10/en_US.twitter.10.txt", SOS = TRUE, EOS = TRUE)
```

```{r}
twitter_unigrams <- CleanTokens(base, n = 1) %>%
  WordFreqProb() %>%
  filter(freq > 1) 

twitter_bigrams <- CleanTokens(base, n = 2) %>%
  WordFreqProb() %>%
  filter(freq > 1)

twitter_trigrams <- CleanTokens(base, n = 3) %>%
  WordFreqProb() %>%
  filter(freq > 1)

twitter_quagrams <- CleanTokens(base, n = 4) %>%
  WordFreqProb() %>%
  filter(freq > 1)
```

```{r}
twitter_model <- CreateFreqTable(twitter_unigrams,
                                 twitter_bigrams,
                                 twitter_trigrams,
                                 twitter_quagrams)
```

## Testing a new prediction algorithm based on freqeuncy model

```{r}
str <- c("sos", "in", "the")

gamma = 0.5

unigrams <- twitter_model %>%
    filter(!is.na(word1), is.na(word2))
  
bigrams <- twitter_model %>%
    filter(!is.na(word2), is.na(word3))
  
trigrams <- twitter_model %>%
    filter(!is.na(word3), is.na(word4))
  
quagrams <- twitter_model %>%
    filter(!is.na(word4))
  
```

```{r}

known_quagrams <- quagrams %>%
  filter(word1 == str[1], word2 == str[2], word3 == str[3], !is.na(word4))
  
trigram_sum <- trigrams %>%
   filter(word1 == str[1], word2 == str[2], word3 == str[3])
trigram_sum <- trigram_sum$freq
  
# P for known quagrams
known_quagrams$P <- (known_quagrams$freq - gamma) / trigram_sum
  
# P-density for unobserved quagrams
alpha1 <- 1 - sum(known_quagrams$P)
  
# all unobserved quagram tails
unob_quagrams <- anti_join(unigrams, known_quagrams,
                           by = c("word1" = "word4"))
  
### Back-off implementation
  
known_trigrams <- trigrams %>%
  semi_join(unob_quagrams, by = c("word3" = "word1")) %>%
  filter(word1 == str[2], word2 == str[3], !is.na(word3), is.na(word4))
  
bigram_sum <- bigrams %>%
  filter(word1 == str[2], word2 == str[3])
bigram_sum <- bigram_sum$freq
  
# P for known trigrams
known_trigrams$P <- (known_trigrams$freq - gamma) / bigram_sum
  
# P-density for unobserved trigrams
alpha2 <- 1 - sum(known_trigrams$P)
  
# all unobserved trigram tails
unob_trigrams <- anti_join(unigrams, known_trigrams,
                             by = c("word1" = "word3"))
  
### ### Back-off implementation
  
known_bigrams <- bigrams %>%
  semi_join(unob_trigrams, by = c("word2" = "word1")) %>%
  filter(word1 == str[3], !is.na(word2), is.na(word3), is.na(word4))
  
unigram_sum <- unigrams %>%
  filter(word1 == str[3])
unigram_sum <- unigram_sum$freq
  
# P for known bigrams
known_bigrams$P <- (known_bigrams$freq - gamma) / unigram_sum
  
# p-density for unobserved bigrams
alpha3 <- 1 - sum(known_bigrams$P)
  
# all unobserved bigram tails
unob_bigrams <- anti_join(unigrams, known_bigrams,
                            by = c("word1" = "word2"))
  
### ### ### Back-off implementation
  
# formatting
unob_bigrams$word2 <- unob_bigrams$word1
unob_bigrams$word1 <- str[3]
  
# probability of unobserved bigrams
unob_sum <- sum(unob_bigrams$freq)
unob_bigrams$P <- alpha3 * (unob_bigrams$freq / unob_sum)
  
### ### ###
  
# collect into one data.table
bigram_predictions <- rbind(known_bigrams, unob_bigrams) %>%
  arrange(desc(P))
  
bigram_predictions$P <- alpha2 * bigram_predictions$P
  
### ###
  
trigram_predictions <- rbind(known_trigrams, bigram_predictions) %>%
  arrange(desc(P))
  
trigram_predictions$P <- alpha1 * trigram_predictions$P
  
###
  
predictions <- rbind(known_quagrams, trigram_predictions) %>%
  arrange(desc(P))
  
predictions
```


```{r}
Predict("sos in the", twitter_model, ngram = 3, gamma = 0.5)
```